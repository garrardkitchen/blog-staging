{"/about/":{"data":{"":" Profileüëã Hey, I‚Äôm Garrard, A Cloud Native Lead at Fujitsu Fujitsu Distinguished EngineerI was awarded this title at the beginning of 2024 I‚Äôm a DevSecOps expert, Cloud Native Lead, and Distinguished Engineer specializing in the Azure hyperscaler ecosystem. With a wealth of experience and multiple certifications including Azure, HashiCorp and Linux foundation, I take pride in bridging the gap between development and operations, ensuring secure, scalable, efficient and sustainable solutions.\nAs a seasoned developer proficient in C# and JavaScript (including TypeScript), with additional experience in Python and Rust, I am deeply passionate about crafting Infrastructure as Code using tools like Terraform and Bicep. I also specialize in orchestrators like Kubernetes, serverless computing, and cutting-edge development with WebAssembly (WASM) outside the browser (OOB), empowering organizations to harness the full potential of automation and cloud-native architecture. Beyond my technical expertise, I am devoted to mentoring aspiring engineers and helping them navigate the ever-changing tech landscape.\nOutside the world of technology, I am a proud father of two energetic boys and a loving husband. This blog is my space to share insights, lessons, and stories from my professional journey and personal life. Whether you‚Äôre here for tech tips, career advice, or a glimpse into my day-to-day experiences, I hope you‚Äôll find something inspiring and valuable.\nThanks for stopping by!"},"title":"About"},"/ai/":{"data":{"":"","#":"Welcome to the home of Artificial Intelligence (AI) content! Here, you‚Äôll find discussions, explanations, and insights into a wide range of AI concepts and technologies. Below is an overview of some key topics, along with their definitions and additional relevant items for exploration:\nLarge Language Models (LLM) Large Language Models are advanced AI models designed to process and generate human-like text at scale. These models, such as GPT (Generative Pre-trained Transformer), are trained on vast amounts of data and are capable of understanding context, answering questions, summarizing text, and even engaging in conversational AI tasks. They excel in a variety of applications, from content creation to customer support and beyond.\nSmall Language Models (SLM) Small Language Models are scaled-down versions of LLMs, typically optimized for specific tasks or domains. While they may not have the expansive knowledge of LLMs, their smaller size allows for faster processing, reduced computational requirements, and easier deployment in resource-constrained environments. SLMs are ideal for targeted applications like chatbots or domain-specific text analysis.\nMachine Learning (ML) Machine Learning is a subset of AI that focuses on creating algorithms and statistical models enabling computers to learn and adapt from data without explicit programming. ML is used in a wide range of applications, from predicting customer behavior to improving medical diagnoses. It forms the foundation of many AI advancements, including neural networks.\nModel Context Protocol (MCP) Model Context Protocol is a framework designed to standardize interactions between different AI models and systems. It enables seamless communication and interoperability by defining how models exchange information and share context. MCP is particularly useful in environments where multiple AI components need to work together efficiently.\nAgent-to-Agent Communication (A2A) Agent-to-Agent (A2A) communication refers to AI systems or software agents interacting autonomously with one another to accomplish tasks or share information. This concept is integral to multi-agent systems, where collaboration and negotiation between agents lead to more complex problem-solving and efficient workflows.\nAdditional Relevant Topics Neural Networks Neural networks are the backbone of many modern AI systems, inspired by the structure of the human brain. They consist of interconnected layers of nodes (neurons) that process and analyze data. Neural networks are used in applications such as image recognition, speech processing, and autonomous driving.\nTypes of Neural Networks Supervised Learning: In supervised learning, the model is trained on labeled data, where the input-output relationships are explicitly provided. Examples include classification tasks (e.g., spam detection) and regression tasks (e.g., predicting house prices).\nUnsupervised Learning: This involves training models on unlabeled data, where the goal is to discover hidden patterns or groupings in the data. A common example is clustering algorithms used for customer segmentation.\nReinforcement Learning: In reinforcement learning, an agent learns to make decisions by interacting with its environment and receiving feedback in the form of rewards or penalties. It is widely used in robotics, game playing (e.g., AlphaGo), and optimization problems.\nNatural Language Processing (NLP) Natural Language Processing focuses on enabling machines to understand, interpret, and generate human language. NLP powers applications like voice assistants, sentiment analysis, and machine translation.\nComputer Vision Computer Vision is the field of AI that enables machines to interpret and analyze visual data, such as images and videos. It is widely used in facial recognition, object detection, and self-driving cars.\nEdge AI Edge AI refers to deploying AI algorithms on edge devices (such as smartphones, IoT devices, or drones) rather than relying on cloud computing. This reduces latency and enhances privacy, making it ideal for real-time applications.\nEthical AI Ethical considerations in AI focus on ensuring the responsible development and deployment of AI technologies. Topics include bias mitigation, transparency, accountability, and the societal impacts of AI.\nThis collection represents just the beginning of what you‚Äôll find here. Whether you‚Äôre new to AI or an experienced professional, there‚Äôs something for everyone to learn and explore. Dive in, and let‚Äôs unravel the fascinating world of Artificial Intelligence together!"},"title":"AI"},"/ai/mcp/":{"data":{"":"","evolution-and-adoption#Evolution and Adoption":"MCP is still in active development, with early adoption by some editor extensions and AI platforms. The protocol is being discussed and refined by the open-source community, with growing interest from major AI tool vendors and IDE developers. As the protocol matures, broader adoption is expected, leading to more seamless and secure AI integrations across tools.\nFor the latest updates, specifications, and community discussions, visit the official Model Context Protocol (MCP) website.\nReferences:\nModel Context Protocol (MCP) Home MCP GitHub Organization Local ","what-is-model-context-protocol-mcp#What is Model Context Protocol (MCP)":"Model Context Protocol (MCP) is an emerging open standard designed to enable AI tools, editors, and services to share and understand the context of a user‚Äôs workspace or project. MCP aims to solve the problem of fragmented context sharing between different AI-powered tools, making it easier for models to provide relevant, accurate, and secure assistance based on the user‚Äôs current environment.","what-problem-does-mcp-solve#What Problem Does MCP Solve?":"Traditionally, AI assistants and code tools operate in silos, each with their own way of understanding project context (such as open files, project structure, or user intent). This fragmentation leads to:\nInconsistent or incomplete context for AI models Redundant or conflicting integrations Security and privacy concerns around context sharing MCP provides a standardized way for tools to communicate context, improving interoperability, privacy, and the quality of AI-powered suggestions. It is based on the client-server architecture, where a host application can connect to multiple servers"},"title":"MCP"},"/ai/mcp/mcp-intro/":{"data":{"conclusion#Conclusion":"MCP is to LLMs what OpenAPI is to RESTful APIs‚Äîa standard for communication, interaction, and integration. By introducing features like Agent-to-Agent communication, MCP addresses key challenges in scalability, standardization, and interoperability. As the adoption of LLMs grows, MCP will play an essential role in shaping how these models interact with each other and the systems they serve.","how-is-mcp-comparable-to-openapi#How is MCP Comparable to OpenAPI?":"OpenAPI is a specification for RESTful APIs that defines how endpoints, methods, and payloads are described. MCP provides a similar structure but is tailored for LLM interactions.\nAspect OpenAPI MCP Purpose Standardizes RESTful API communication Standardizes communication for LLMs Describes Endpoints, methods, payloads, responses Prompts, responses, behaviors of LLMs Format JSON or YAML JSON-based protocol Machine-readable Yes Yes Interoperability Enables systems to integrate with APIs easily Enables systems and LLMs to work together Developer Benefits Clear API contracts, faster integration Consistent LLM interaction, easier scaling In essence, MCP acts as the ‚Äúcontract‚Äù for how LLMs interact with other systems, just as OpenAPI does for REST APIs.","problems-mcp-solves#Problems MCP Solves":"1. Lack of Standardization Currently, every LLM has its own unique API and interaction model. MCP provides a universal protocol, reducing the need for custom integrations. 2. Complex LLM Ecosystems In systems with multiple LLMs or agents, communication can become fragmented. MCP and A2A enable a cohesive framework for inter-agent communication. 3. Scalability Challenges Scaling LLM systems often involves ad-hoc solutions for orchestration. MCP introduces modularity, making it easier to scale and manage these systems. 4. Developer Friction Developers often face difficulty in integrating different LLMs. MCP simplifies this process by providing a clear and consistent interface. ","understanding-mcp-and-its-comparison-to-openapi-for-llms#Understanding MCP and Its Comparison to OpenAPI for LLMs":"When I first came across the Model Context Protocol (MCP), I couldn‚Äôt help but notice its parallels with OpenAPI. What follows is my attempt to connect the dots. While it‚Äôs not a perfect one-to-one comparison, it‚Äôs fascinating how concepts can feel similar simply by swapping a few key terms.\nUnderstanding MCP and Its Comparison to OpenAPI for LLMsIn the world of APIs, OpenAPI has become the gold standard for describing RESTful APIs in a way that is machine-readable, consistent, and developer-friendly. Similarly, the Model Communication Protocol (MCP) aims to provide a standardized framework for Large Language Models (LLMs) to interact with applications, enabling interoperability, scalability, and ease of integration. This document explores how MCP is comparable to OpenAPI, introduces the concept of Agent-to-Agent (A2A) communication, and examines the problems MCP solves in the LLM ecosystem.","what-is-agent-to-agent-a2a-communication#What is Agent-to-Agent (A2A) Communication?":"Agent-to-Agent (A2A) is a critical component of MCP. It refers to the ability of multiple agents (LLMs or other AI systems) to communicate with one another in a standardized manner.\nWhere A2A Fits into MCP: Collaboration: A2A allows different LLMs or agents to collaborate on tasks by sharing information and results. Decentralization: By enabling direct agent communication, A2A reduces the reliance on a centralized system. Scalability: A2A helps scale systems by distributing tasks across multiple agents or models. Example Use Case: Imagine a customer support system where:\nAgent 1 specializes in answering technical questions. Agent 2 focuses on billing inquiries. Agent 3 is a general-purpose assistant. With A2A, these agents can coordinate:\nAgent 3 receives a query and determines it‚Äôs a billing issue. It forwards the query to Agent 2 using MCP protocols. Agent 2 processes the query and returns the response to Agent 3, which relays it to the user. ","what-is-mcp#What is MCP?":"MCP, or Model Communication Protocol, is a standard designed to facilitate seamless interaction between LLMs and external systems. It provides a structured way to define how LLMs can:\nUnderstand requests from external applications. Respond in a consistent format that applications can parse. Allow multiple LLMs or agents to communicate with each other (Agent-to-Agent communication). Core Goals of MCP: Interoperability: Ensure different LLMs can work within the same ecosystem. Standardization: Provide a protocol for defining inputs, outputs, and behaviors. Scalability: Enable the expansion to more complex systems with multiple agents. Modularity: Allow developers to plug in or swap out LLMs easily. "},"title":"Comparison to OpenAPI"},"/ai/mcp/mcp-local/":{"data":{"":"","links#Links":" Link Description Click here I wanted to see for myself how easy it is to create an MCP Server (albeit running locally). I am keen to start my personal journey and the way to communicate context, improve interoperability, privacy and enure quality of AI-powered suggestions. new "},"title":"Local MCP Server"},"/blog/":{"data":{"":" RSS Feed "},"title":"Blog"},"/blog/apache-ignite-running-agent-with-dotnetcore-server-node/":{"data":{"":"I‚Äôve recently been researching into Apache Ignite. Apache Ignite is an in-memory, memory-centric, distributed database, caching and processing platform for transactional, analytical, and streaming workloads.\nSo why the post? Well, with using .NET Core, I have run into one or two challenges that I have had to work through. One of which involves the Agent. I feel it is important to share with you how I get beyond this issue. It may save you a lot of time if you‚Äôre an Apache Ignite noob like me.\nYou use the Agent when you want to execute queries, SQL DML \u0026 DDL amongst other actions, from within the Web Console app. The Agent acts as a proxy. The Agent must connect to both the Web Console and your Server node or Thick Client node.\nAs I say above, this Agent acts as a proxy between the Web Console (UI to configure clusters, execute SQL DML \u0026 DDL, and query KV stores including visuals on caches etc‚Ä¶) and a Server node (aka, data node) to execute SQL \u0026 KV stores.\nWith all previous efforts, I was not able connect the Agent to the data node when the data node was created using a .NET runtime. When running the same configuration but using Java instead, it would connect without issue. Based on being able to connect to data node when created via java, I was confident that I would find a way to get this to work.\nAfter many hours of twawling the internet for answers and failed attempts, I figured out what the issue was. The Apache.Ignite nuget package does not include the ignite-rest-http module (contains many Jars), and this is what the Agent needs to communicate with the data node. So, what you need to do is download the entire Ignite binary package, and extract the ignite-rest-http folder. Then you use the following code to add a list of comma-separated file names of the HTTP Jar files to the IgniteConfiguration.JvmClasspath property:\nvar cfg = new IgniteConfiguration { JvmClasspath = Directory.GetFiles(pathToIgniteRestHttpJars) .Aggregate((x, y) =\u003e x + \";\" + y) }; using (var ignite = Ignition.Start(cfg)) { ... From what I could find, there‚Äôs no plans on including the ignite-rest-http module in the Apache.Ignite nuget package.\nI will share a GitHub repo to ease you into this shortly."},"title":"How to run the Apache Ignite Agent with an Ignite.NET Core Server Node"},"/blog/azure-defender-for-cloud/":{"data":{"azure-blade#Azure blade":"","defender-for-cloud-containers#Defender for Cloud Containers":"","gh-action-job#GH Action Job":"Defender for Cloud ContainersSetting up Defender for Cloud Containers to work with your CICD pipeline is quick and uncomplicated. I do not walk through these set up steps in this post. For that, you can follow those few steps here in this Microsoft post instead ‚û° Setup. The goal of this post is to highlight a few areas of interest and to share my opinions on this feature. I have understandably obfuscated sensitive information.\nTL;DR:\nPros:\nRapid inclusion in your GH Actions CICD pipeline Uncomplicated Adherence to industry preferred practice Cons:\nExpected ALL findings from GHA run to be visible in Defender blade Doesn‚Äôt support windows containers (.NET Framework workloads) üò± substantiated here ‚û° Availability Set upIt takes very little time to configure image scanning and to secure your container images. In my current role as Head of Cloud Platform at Carfinance 247 I am spear heading the migration effort to move our entire workload real estate from on-premise to Azure. As part of this mission, we‚Äôre using GH Actions for our CICD pipelines. Azure Defender of Cloud Containers compliments GH Actions and I personally have found it a very painless exercise 1. It takes little more time than it does to actually read their instructions to configure, run and see the scan summary and remediation advice.\nWith regards to configuring Azure Defender for Cloud, all it takes a few mouse clicks and you‚Äôre done. During this process, you will be required to copy 2 values it makes available to you that will need to be added as GitHub secrets.\nThe final step is to insert 2 GH Actions into your GHA Workflow. You may need to seperate your build and push steps as per their instructions. Below is a snippet from one of our deploy GH Actions Workflows that shows how we‚Äôve incorporated these GH Actions:\n... - name: BUILD IMAGE run: | cd ${{ env.ROOT_DIR }} docker build -t ${{ env.ACR_NAME }}/${{ env.APP_DOCKERIMAGE }}:${{ env.TAG }} -f ${{ env.ROOT_DIR }}/${{ env.APP_DOCKERFILE }} . errorCode=$? if [ $errorCode -ne 0 ]; then echo \"Could not build to ACR, error occured with docker build\" exit 1 fi - name: SCAN FOR VULNERABILITIES uses: Azure/container-scan@v0 id: container-scan continue-on-error: true with: image-name: ${{ env.ACR_NAME }}/${{ env.APP_DOCKERIMAGE }}:${{ env.TAG }} - name: PUSH TO ACR run: | docker push ${{ env.ACR_NAME }}/${{ env.APP_DOCKERIMAGE }}:${{ env.TAG }} errorCode=$? if [ $errorCode -ne 0 ]; then echo \"Could not build to ACR, error occured with docker build\" exit 1 fi - name: POST LOGS TO APPINSIGHTS uses: Azure/publish-security-assessments@v0 with: scan-results-path: ${{ steps.container-scan.outputs.scan-report-path }} connection-string: ${{ secrets.AZ_APPINSIGHTS_CONNECTION_STRING }} subscription-token: ${{ secrets.AZ_SUBSCRIPTION_TOKEN }} ... üëÜ We are not using (AZ_APPINSIGHTS_CONNECTION_STRING, AZ_SUBSCRIPTION_TOKEN) secrets names\nResultsThere are 2 places where you can view the Commons Vulnerabilities and Exposures. These locations are wihtin GH Actions and the Defender for Cloud Blade in the Azure portal.\nGH Action Job ","references#References":" Setup defender for container registries\nDefender for cloud","results#Results":"","set-up#Set up":"","summary#Summary":"In summary:\nI found it quick to get up and running and see the CVEs and remediation advice 1 - Like with most processes, IMO, if there‚Äôs no predetermined action to react to findings - e.g. automated Runbook - then it‚Äôs a pointless exercise and your organisation will remain exposed to such vulnerabilities IMO, this has be part of a wider initiative. For example, the inclusion of code quality analysis is a must to avoid vulnerabilities, bugs and poor coding practices/implementations making it into the codebase in the first place. This is how we roll. GitHub makes this easy! I find it surprising how many CVEs, irrespective of criticality, are present in established docker images. You don‚Äôt have to look far to discover them! "},"title":"Azure Defender for Cloud"},"/blog/compare-resources-from-terraform-plan/":{"data":{"":"Here‚Äôs a link to a repo I created today, that will be used to host examples of all challenges I encounter that relate to Terraform.\nIn this particular sample, I need to list out all those resources that will be created using Terraform where we do not have a state file. The product of this will be added to sheets, by Resource Group, in an Excel spreadsheet. Each sheet is an Azure Resource Group. The rationale for this is that at some point an apply was executed, and due to the state not being managed, we do not know if the current config matches with what has been deployed.\nThe Excel spreadsheet gives us something organised and visual to use to compare. This will also be used to confirm the correct naming convensions have been used.\nThis is an example of the resulting Excel spreadsheet:\nTo see the the few LOC used to generate this, please click here."},"title":"Compare Resources From Terraform Plan"},"/blog/digital-certificates/":{"data":{"asymmetric-encryption#Asymmetric encryption":"","create-a-rsa-private-key#Create a RSA private key":"","decrypt-the-message#Decrypt the message":"","digital-certificates#Digital Certificates":"","encrypt-the-message#Encrypt the message":"","extract-public-key#Extract public key":"","generate-a-digital-signature#Generate a digital signature":"","how-is-the-signature-verified#How is the signature verified?":" Token From encoded token\nAlgorithm (HASH_1):\nENCRYPT ( KEY -\u003e ( HASH ( base64 (header) + \".\" + base64 (payload) ) ) ) The Signature is RSA SHA of ( base64(header) + ‚Äú.‚Äù + base64(payload)).\nHere, in the jwt.io site, it is recalculated after each valid payload change.\nKey pair Comparison using key pair\nAlgorithm (HASH_2):\nDECRYPT ( KEY -\u003e ( HASH ( base64 (header) + \".\" + base64 (payload) ) ) ) It base64 encodes the header + payload using the key pair, then encrypts it. If this matches the signature in the the encoded token then the signature is verified:\nVERIFIED = HASH_1 == HASH_2\nAs soon as I paste in my public and private keys, it correctly verifies the digital signature:","jwt#JWT":"I‚Äôve been wanting to put some notes down on digital certificates, signing and JWT for some time now. I find there are plenty of confusing terms involved in this area, plus a few nuances that have added to my personal confusion. I feel it now important to document these before I forget and move on [to another project].\nSo, what‚Äôs triggered this post? Well, one of many tasks I‚Äôm involved in [juggling] evolves SSO (single sign on). Albeit, mainly focused on the architecture on this task, I have compiled a few PoCs where I‚Äôm using digital certificates for authentication. In particular, SSOing into Twilio Flex and using an identity field returned from their I.AM service, to seamlessly log into our internal CRM, securely using a digital certificate.\nTermsOk, let‚Äôs start with a few terms. I‚Äôll slowly integrate these terms in the following examples.\nKeys\nA key is something that is used to encrypt a piece of data (think JWT payload). It can be phrase (series of characters) or a public/private key held in a digital certificate.\nHash\nA hash is a piece of data, that cannot be reengineered to reveal it‚Äôs original content, also referred to as a digest or one-way hash.\nSHA (Secure Hashing Algorithm)\nIt is for cryptographic security. It is used to produce an irreversible and unique hash.\nEncryption\nThe process of converting something to gobbledygook and only be able to read it when you have the key used when it was encrypted.\nDigital signature\nThe encrypted hash, that proves the data has not been tampered with in-flighted AND verifies the identity of the entity presenting it.\nSigning\nThe process of creating the digital signature.\nBase64\nThe more efficient was of encoding and sending data over a network.\nCipher algorithm\nA cipher algorithm is a mathematical formula designed specifically to obscure the value and content of data. Most valuable cipher algorithms use a key as part of the formula. This key is used to encrypt the data, and either that key or a complementary key is needed to decrypt the data back to a useful form.\nRSA (Rivest‚ÄìShamir‚ÄìAdleman)\nRSA is one of the first public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and distinct from the decryption key which is kept secret (private).\nSymmetric Encryption\nSymmetric encryption is a type of encryption where only one key (a secret key) is used to both encrypt and decrypt electronic information.\nAsymmetric Encryption\nAsymmetric Encryption is a form of Encryption where keys come in pairs. What one key encrypts, only the other can decrypt.\nX.509\nIs a standard format for public key certificates. Each X.509 certificate includes a public key, identifying information, and a digital signature.\nOf course, if this [digital signature] is new to you, the above won‚Äôt (yet) make much sense.\nI‚Äôm going to walk you through an example, well 2 actually. One that used a phrase as a key(aka keyphrase), and the other that used a public/private key found in a digital certificate (albeit, self-signed). I am going to use a tool call openssl, not may have heard of it?\nSymmetric encyrptionEncryption using a keyphrase\nIn this first example, I‚Äôm going to encrypt a message with a keyphrase.\nBefore I begin, I‚Äôm going to write the content of my secret message to a file called msg.txt.\nNext, I‚Äôm going to encrypt this file it using a keyphrase of abc123 and output the encrypted file to msg.txt.enc:\n$ openssl enc -e -aes256 -k abc123 -in ./msg.txt -out ./msg.txt.enc above you‚Äôll see -aes256. This is the cipher algorithm we‚Äôre using\nThe encrypted file looks something like this:\nSalted__BmFÔøΩj‘ø≈ÄaÔøΩÔøΩ1ÔøΩ\u0001\u001cÔøΩ\\X ÔøΩÔøΩÔøΩ{'VÔøΩ\u001bd\u001aFq\u0018\u0007ÔøΩ\u0026ÔøΩÔøΩÔøΩLÔøΩ8:ÔøΩ\u0017\u0002ÔøΩÔøΩÔøΩÔøΩ\bÔøΩT Pure gobbledygook!\nNow, I‚Äôm going to decrypt this encrypted file msg.txt.enc and output the encrypted file to msg.txt.dec. It is imperative that I used the same keyphrase:\n$ openssl enc -d -aes256 -k abc123 -in ./msg.txt.enc -out ./msg.txt.dec The decrypted file msg.txt.dec looks like:\nmy secret message If I had omitted the keyphrase, I will have been prompt for it which will have looked like this:\n$ openssl enc -d -e -aes256 -in ./msg.txt.enc -out ./msg.txt.dec enter aes-256-cbc decryption password: Or, if I had used the incorrect `keyphrase, I will have seen something like this:\nbad decrypt 140120216352064:error:06065064:digital envelope routines:EVP_DecryptFinal_ex:bad decrypt:../crypto/evp/evp_enc.c:583: A good simple illustration of how to encrypt and decrypt a message using openssl enc command.\nAsymmetric encryptionEncryption using a public/private key\nIn this section I‚Äôm going to:\nGenerate a RSA private key Extract the public key from the private key Generate a hash of the data I want to send, as well as signing it (using private key) Encrypt the data I want to send Decrypt the data I have received Verify the signature of the data received - ensure it data wasn‚Äôt tampered with in-flight Let‚Äôs first generate the message I want to securely transmit:\n$ echo 'my secret message' \u003e msg Create a RSA private key Here I‚Äôm using the genrsa command. This command generates an RA private key:\n$ openssl genrsa -out private.pem 4096 $ openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 Extract public key $ openssl rsa -in private.pem -pubout -out public.pem Generate a digital signature using dgst Here, I‚Äôm generating a hash (digest) of the message as well as signing it with the private key\n$ openssl dgst -sha256 -sign private.pem -out msg.signature msg using rsautl rsautl, unlike dgst, does not create a hash or ASN1 encoding.\nüö´ As rsautl uses the RSA algorithm directly, it can only be used to sign, or verify, small pieces of data: $ openssl rsautl -sign -in msg -inkey private.pem -out msg.sig Encrypt the message The rsautl command can be used to sign, verify, encrypt and decrypt data using the RSA algorithm.\n$ openssl rsautl -encrypt -inkey public.pem -pubin -in msg -out msg.enc By including the -pubin switch, you‚Äôre telling the command that the input key file (-inkey) is an RSA public key. Withou this, it assumed you‚Äôre using a private key\nDecrypt the message $ openssl rsautl -decrypt -inkey private.pem -in msg.enc -out msg.dec Verify signature using dgst This uses the public key to decrypt the Hash of the original msg:\npseudo logic:\nhash_1 = Hash ( msg ) hash_2 = Dec ( Key -\u003e Hash ) IsVarified = hash_1 == hash_2 $ openssl dgst -sha256 -verify public.pem -signature msg.signature msg Verified OK using rsautl This verifies the original message using the signature and outputs it:\n$ openssl rsautl -verify -inkey private.pem -in msg.sig my secret message Digital CertificatesTo verify the identity of the entity presenting it\nSo far, we‚Äôve covered hashes, key pairs, digital signatures and encryption and decryption. This section is where I cover, briefly, digital certificates. I‚Äôm using a digital certificate to replace the key pair as covered above and to give the capability of using additional information to verify that the identity of the entity presenting this message.\nLet‚Äôs start by creating a self-signed certificate. Type:\n# create self-signed certificate openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:4096 -keyout myserver.pem -out myserver.crt -subj \"/C=UK/OU=IT/CN=myserver.com\" We can inspect the content of the certificate by typing:\nopenssl x509 -in myserver.crt -text -noout For bravity, I‚Äôm using the same commands as used above to; extract public key, generate digital signature, encrypt/decrypt then verify the signature. I‚Äôve included all the statements in one block:\n# extract public key from self-signed certificate openssl rsa -in myserver.pem -pubout -out server-public.pem # generate hash and sign (digital signature) $ openssl dgst -sha256 -sign myserver.pem -out msg.server-signature msg # encrypt my message openssl rsautl -encrypt -inkey server-public.pem -pubin -in msg -out msg.enc2 # decrypt my encrypted message openssl rsautl -decrypt -inkey myserver.pem -in msg.enc2 -out msg.dec2 # vertify signature openssl dgst -sha256 -verify server-public.pem -signature msg.server-signature msg This results in:\nVerified OK JWTSo, how does the digital signature relate to the Signature verification against a JWT Token?\nIn this section I will be using the jwt.io website. From this site I can choose which cipher algorithm. I will be using a RSA (widely used for secure data transmission and public-key cryptography) cipher as I‚Äôm simulating the sending and receiving of a JWT Token over HTTP.\nA JWT signature will use RSA SHA (irreversible hash) of the header and payload. This algorithm is set in the header so we have all the information we need to decrypt the encrypted data. However, we‚Äôre not able to verify these points yet: (a) has the message been tampered with inflight and (b) the identity of the entity presenting this message.\nIn actual fact, you will see this if you copied the a JWT token without keys into jwt.io (selecting RSA256 algorithm). It will show Invalid Signature. So, to verify these points, you need to provide the public and private key. It will use the private key to obtain the original Hash (hash of the original data) then decrypt this. If once decrypted, this equates to the RSASHA246 HMAC, then the signature is verified.\nAll I‚Äôve done is added a tenant property to the claims (payload). I‚Äôve doing this prove that I‚Äôve changed the claim and will become apparent shortly why I‚Äôve done this:\n{ \"sub\": \"1234567890\", \"name\": \"John Doe\", \"admin\": true, \"iat\": 1516239022, \"tenant\": \"foobaa\" } Next, I copy in the public and private key into the verify signature area. This all looks like this:\nI copy the encoded token (will paste it back in, in a moment) then refresh the page. The page is recent and defaults loaded (observe the changed payload):\nI now copy in my encoded token:\nYou will see the Invalid Signature near the bottom, but, the correct payload is back! This is because the certificates key pair in this default screen are different to my digital certificate‚Äôs key pair.\nSo, if I removed the entire encoded signature from the encoded token, we‚Äôll still see the decrypted payload:\nSo, how‚Äôs it validating the signature? ‚Ä¶","jwt-token-format#JWT Token format":"Let‚Äôs remind ourselves of the structure of a JWT Token is:\n{header}.{payload}.{signature}\nThe signature, with using the RS246 cipher, is a RSA SHA of the header (just cipher algorithm \u0026 type, which is JWT) AND payload. This simply means it is using a public-key (from our digital certificate) to encrypt a one-way hash of our original message (header + payload).","references#References":" openssl genrsa openssl rsautl openssl examples the difference rsautl -sign AND dgst -sign openssl ","symmetric-encyrption#Symmetric encyrption":"","terms#Terms":"","verify-signature#Verify signature":""},"title":"Digital Certificates"},"/blog/dotnet-aspire-and-redis/":{"data":{"addendum#Addendum":"I added this section a few days after posting the original article\nWell, I couldn‚Äôt wait to use AZD to deploy my sample app. Sadly, this didn‚Äôt go too well.\nI got this error [condensed] at the deploy application stage:\nüö´ error CONTAINER1013: Failed to push to the output registry: The request was canceled due to the configured HttpClient.Timeout of 100 seconds elapsing I managed to deploy most of the infra plus 2 ACAs (apiserver and cache) with the azd up command but failed to deploy webfrontend. On retry, it fails this time with apiservice which it had previously deployed successfully. I retried with azd deploy webfrontend but I saw a repeat of the the above.\nI found this GH issue that confirms this behaviour ‚û°Ô∏è https://github.com/Azure/azure-dev/issues/3225 and that it‚Äôs yet to be addressed.\nSo, sadly you can‚Äôt use AZD with .NET Aspire, yet. It‚Äôs a real shame this hasn‚Äôt been made public.","conclusion#Conclusion":".NET Aspire appears to offer that simplicity glue that is very much missing and I must admit, it does looks great from an inner loop perspective. I‚Äôm yet to evaluate its CD approach and how it handles the provisioning of the supporting infrastructure. I see from Deploy a .NET Aspire app using AZD that both bicep and Terraform are supported in that scernario. This will be my next step. I am also reassured that the tech that I‚Äôm using on various projects, as a direct result of my research, such as bicep, TF, AZD and ACA, is being utilized here as well. I do hope the wow and positive DevEx continues.","confirmation-of-counter-persistence#Confirmation of counter persistence":"I couldn‚Äôt leave it there. I had to make sure the counter was being written to the cache. Nothing fancy, all I did was I exec‚Äôed into the running container, and then ran a few cli commands.\nTo get the current value of the counter, use:\nredis-cli hget counter \"data\" I also update the counter then refresh the page to see it reflect this change:\nredis-cli hset counter \"data\" 25 I also looked at the distributed tracing to get confirmation of this. Here‚Äôs an example of this:\nDistributed trace example And that was that. I was able to take a sample Aspire app and perist a counter value to Redis cache with very little effort. All in all, this was a positive DevEx.","dependency-injection#Dependency Injection":"I then added this Redis Client to the service collection, again, in the this sample1.Web‚Äôs Program.cs file:\nbuilder.Services.AddScoped\u003cRedisClient\u003e(); ","first-look-into-net-aspire#First look into .NET Aspire":"","persisting-a-counter-to-redis-cache#Persisting a Counter to Redis cache":"Ref: https://learn.microsoft.com/en-us/dotnet/aspire/fundamentals/setup-tooling?tabs=visual-studio","razor-page#Razor page":"The last piece to this jigsaw is the counter razor page. There‚Äôs a few different pieces to do here so to help I‚Äôve decomposed these to steps.\nStep 1: Inject this new RedisClient into the page:\n@inject RedisClient RedisClient Step 2: Overwrite the page‚Äôs OnInitializedAsync implementation in order for us to be able to get our counter from our redis cache:\nprotected override async Task OnInitializedAsync() { currentCount = await RedisClient.GetCounterAsync(\"counter\"); } Step 3: We now need to update this counter when the button is pressed:\nprivate async Task IncrementCount() { currentCount++; await RedisClient.SetCounterAsync(\"counter\", currentCount); } This is what the resulting page looks like this:\n@page \"/counter\" @rendermode InteractiveServer @inject RedisClient RedisClient \u003cPageTitle\u003eCounter\u003c/PageTitle\u003e \u003ch1\u003eCounter\u003c/h1\u003e \u003cp role=\"status\"\u003eCurrent count: @currentCount\u003c/p\u003e \u003cbutton class=\"btn btn-primary\" @onclick=\"IncrementCount\"\u003eClick me\u003c/button\u003e @code { private int currentCount = 0; protected override async Task OnInitializedAsync() { currentCount = await RedisClient.GetCounterAsync(\"counter\"); } private async Task IncrementCount() { currentCount++; await RedisClient.SetCounterAsync(\"counter\", currentCount); } } I felt that what I had done was logical and flowed well. I do have experience [limited] with Blazor pages so I did go into this feeling confident and that this was not going to be too much of a stretch.\nJust like that, I was all set to run the app! I‚Äôm delighted to announce it worked flawlessly and as intended, like most of the code I write üëÄ.","redis-dependency#Redis Dependency":"First off, I needed a new pre-release NuGet package for this to work:\ndotnet add package Aspire.StackExchange.Redis.DistributedCaching --prerelease Next I added this so the necessary types get added to the service collection, by adding the following to Program.cs in sample1.Web :\nbuilder.AddRedisDistributedCache(\"cache\"); ","redisclient#RedisClient":"I then created a Redis Client that I could use to call from the counter razor page to obtain and update the counter value. This is that code:\nüëâ I‚Äôve not chosen to use the incr type\nüëâ Redis stores these string as a hash types\nusing Microsoft.Extensions.Caching.Distributed; namespace sample1.Web; public class RedisClient(IDistributedCache cache) { public async Task\u003cint\u003e GetCounterAsync(string key) { var value = await cache.GetStringAsync(key); return value == null ? 0 : int.Parse(value); } public async Task SetCounterAsync(string key, int value) { await cache.SetStringAsync(key, value.ToString()); } } ","references#References":" https://github.com/dotnet/aspire-samples https://learn.microsoft.com/en-us/dotnet/aspire/fundamentals/setup-tooling?tabs=visual-studio https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/ https://learn.microsoft.com/en-us/dotnet/aspire/deployment/azure/aca-deployment-azd-in-depth ","the-wow#The \u003ccode\u003eWow\u003c/code\u003e":"First look into .NET AspireTL;DR: See here for why AZD is not yet supported.\nI decided to give .NET Aspire a try. I‚Äôm yet to watch an online tutorial but I did watch its announcement a few months ago. At the time I did think YAGNI. For me, it didn‚Äôt make much sense. Either that, or it just wasn‚Äôt being explained well enough. Enthusiasm alone isn‚Äôt enough to promote a new tool. Since then, I‚Äôve seen many tweets. So, over the last month or so, I‚Äôve gradually grasped its raison d‚Äô√™tre.\nLast night, with some time to kill, I asked Copilot to send me some YT links so I could learn more. But Copilot refused to share any YT links. It only gave me some MS Learn links. One of them took me to the Samples GH repo.\nThe first time I opened a sample project, the Visual Studio Community 2022 (Preview) version of VS I was using, asked me to install an Aspire component. It turned out to be the Aspire workload. It was a simple click-and-forget operation. I wish everything in life was that easy.\nTo avoid this ahead of time you can:\n‚ÑπÔ∏è .NET Aspire workload\nInstall the .NET Aspire workload\ndotnet workload install aspire or update all workloads\ndotnet workload update then to confirm version and install status\ndotnet workload list The WowThe sample I tried to run in Visual Studio did not work, so I switched to another one. Then I saw the dashboard on my personal developer laptop for the first time. I was impressed (a bit). I explored the metrics, logs, and traces. I was more impressed.\nI was even more impressed when I saw an example solution that had both a Windows Forms and a WPF app. I wanted to test how easy it was to set up and find a situation that would require me to code.\n‚ö†Ô∏è My sample repo\nYou can find my sample .NET Aspire applicaton here: https://github.com/garrardkitchen/dotnet-aspire-and-redis-sample\nTo create your first .NET Aspire application is straight forward. From the menu you choose Create a new project. I filtered on the .NET Apsire project type just to see what‚Äôs available:\n.NET Aspire project templates I selected the .NET Aspire Starter Application and named it sample1. I was prompted to start up Docker for Desktop due to the Redis dependency. The next action I performed was to debug the app to see if anything barked at me. It didn‚Äôt and I was presented with the dashboard:\n.NET Aspire dashboard I clicked on the Endpoint link next to the frontendweb project and this Blazor app loaded:\nMy first Aspire hosted sample app I clicked on the Counter menu option, then the click me button which incremented the on page counter. I clicked away and when I returned the counter had zeroed. Great, I thought, this is where I can start hacking away to see how easy it is to persist this to the redis cache. The remainder of this post covers what I did."},"title":".NET Aspire and Redis"},"/blog/dotnet-stack-and-heap/":{"data":{"":"This week I have been investigating how to reduce memory allocation in a few HTTP APIs. I won‚Äôt go into any explicit work-related examples here but I will touch on facets relating to this effort.\nLet‚Äôs start off by looking at Reference Types and Value types and how they get allocated into the Heap. I will also touch on concepts such as boxing and GC pressure.\nLet me start off with some facts:\nA Reference types always get allocated on the Heap A Value type mostly get put on the stack, but do get placed on the Heap sometimes. If Value type is a class field (hoisted top-level class field), this gets allocated on the Heap along with it‚Äôs parent. If Value type is boxed, this too gets allocated on the Heap A ref Struct will always be added to the Stack as the compiler will not allow it (field cannot be of byref-like type) to be added as a class field, but Value type in the Struct will be allocated if boxed Yikes, confusing yes?","concepts#Concepts":"Boxing and unboxing\nBoxing is process of converting a Value Type to the type object (aka implicit conversion). It creates a new allocation in the Heap, copies in the Values type value and returns a reference.\nSee the last int32 instance in this screengrab:\nUnboxing, on the hand, is the reverse and is the process of converting a type object to a Value Type (aka explicit conversion).\nIn this example, you see that the unboxing doesn‚Äôt get allocated onto the Heap:","examples#Examples":"I‚Äôm going to break off into some examples next to help explain why and when Value Types get allocated.\nExample 1: A Class Field\nIn this class, we can see that _attempts is a Value Type:\npublic class NewOrder { private int _attempts = 0; public void PlaceOrder() {} } Due to it being a class field, it is allocated on Heap with it‚Äôs parent NewOrder. This is seen here:\nThe same will happen if you hoist a Struct to the root of the class. Take this code for instance, the Item Struct is placed on the Heap along with it‚Äôs parent (see screengrab below):\npublic class Order { private Item _item = new Item(); public Order() {} } public struct Item { public int Id { get; set; } public string Sku { get; set; } } Example 2: Boxing\nClass Properties Value Types don‚Äôt get placed on the Heap. However, if they get boxed (eg via string interpolation, method group, lambda expression), then they do.\nThis example shows the result on the ItemCount property after it gets boxed via string interpolation:","gc-pressure#GC Pressure":"You may be asking the question, ‚Äúwhy is any of this important?‚Äù\nOne reason it will become relevant is if you are observing GC pressure.\nGC pressure means that the GC is feeling the strain and increasingly becoming overwhelmed deallocating memory. This could be the result of an incorrect GC configuration. If you‚Äôre not careful, your production docker container workload(s) may not have adeqaute available private memory. If this is the case then GC will be working twice as hard to avoid an OOM exception. Lack of memory will kill your app. Lack of CPU however will simply throttle your app and will not result in your app being kill. There is one example when this isn‚Äôt exactly true. Let‚Äôs say you have a pod running in your kubernetes cluster, and your configuration includes both Liveness and a Readiness probes. If your CPU is maxed out and your HTTP Listener can‚Äôt receive and respond to HTTP requests during the time the probes properties allow (combination of periodSeconds, failureThreshold and timeoutSeconds), then neither probe will have the availability to inform AKS that it‚Äôs still alive but just busy so don‚Äôt shut me down. So, the inevitable will happen. Yes, AKS will kill your pod and restart another; providing you‚Äôve a ReplicaSet configured.\nReviewing your code and identifying changes that can reduce allocation, will help. It‚Äôs not the only approach. More often than not though, especially with a focus on using less space (Big O Notation), will result in larger method frames, plus verbose code, plus determinate collection sizes, etc‚Ä¶ .\nAlso worth noting here too is that whenever GC executes, your running application will stop and will resume once GC completes.\nTo reiterate an earlier point, there are many approaches to reducing GC pressure. These are well documented and are easily found on the internet. I‚Äôve included several below for completeness:\nAvoid memory leaks (big topic!) Use (when appropriate) in place of a Class Using a StringBuilder correctly Use .NET 6 \u0026 c# 10. String interpolaton uses the new DefaultStringInterpolatedHandler, avoiding unnecessary boxing on Value Type values. Avoid finalizers Setting the initial seize of a dynamic collection ArrayPool for short-lived arrays (large) As a side note here due to the inclusion of the .NET 6 and string interpolation point above, I‚Äôve now been using .NET 6 for a few months. This includes both exploratory and new projects. I was especially keen to start using .NET 6 from the benefits from process isolation (out-of-process) - fewer conflicts, DI and full control of the process that we‚Äôre all used to with paradigms outside of the serverless model. I have grown to like the minimum API. Like most, I initially felt uneasy with the lack of c# verbosity but now welcome it. I do, and I am sure I am not alone here, have been using both BenchmarkDotNET and SharpLab to compare performance and language decompilation. I didn‚Äôt bother much with .NET 5. This was due to .NET 5 never having the LTS label.","references#References":" Boxing/unboxing "},"title":".NET Stack, Heap and Boxing"},"/blog/error-netsdk1045/":{"data":{"conclusion#Conclusion":"If you get Error NETSDK1045 in Visual Studio then upgrade to the latest verison of VisualStudio.","error#Error":"A colleague had Visual Studio shout this (see üëá) at him when he loaded up a FunctionsApp project. He received this error twice as the second project was the unit tests for the FunctionsApp.\nCaution\nError NETSDK1045 The current .NET SDK does not support targeting .NET 6.0. Either target .NET 5.0 or lower, or use a version of the .NET SDK that supports .NET 6.0. TestNasLinuxFuncAppWebTests C:\\Program Files\\dotnet\\sdk\\5.0.409\\Sdks\\Microsoft.NET.Sdk\\targets\\Microsoft.NET.TargetFrameworkInference.targets 141\nI interpretted this as the newer targets were not supported by the existing SDK. Yeah, genius right üòÅ.\nHowever, even after installing .NET 6.0 and the Azure Functions Core Tools using it didn‚Äôt fix the issue:\nwinget install -e --id Microsoft.DotNet.SDK.6 winget install -e --id Microsoft.AzureFunctionsCoreTools So, like most at this stage, I consulted with a trusted colleauge (Google) and found this in the MS Documentation - https://learn.microsoft.com/en-us/dotnet/core/tools/sdk-errors/netsdk1045. He did reboot his vm, yet the error remained.\nI did also find a SO suggesting upgrading to VS2022 would fix this issue.\nI couldn‚Äôt find the reciprocal recommendation in MS documentation but he did upgrade to vs2022 regardless. This did the trick. ü•≥","error-netsdk1045-the-current-net-sdk-does-not-support-targeting-net-60#\u003cem\u003eError NETSDK1045 The current .NET SDK does not support targeting .NET 6.0\u003c/em\u003e":"Error NETSDK1045 The current .NET SDK does not support targeting .NET 6.0"},"title":"Error NETSDK1045"},"/blog/github-action-workflow-starter/":{"data":{"":"In this post, I share practical insights on how to simplify and streamline your CI/CD processes using GitHub Actions. The focus is on reducing complexity, minimizing effort, and making it easier for teams to adopt new workflows with less cognitive overhead.","an-anecdote#An anecdote":"During my time working with different teams, there have been moments of differing opinions about tooling choices. For example, when GitHub Actions was adopted, some colleagues remarked on my enthusiasm for the platform. For me, the motivation has always been to select tools and processes that best serve the needs of the organization and the development team as a whole. Like many in the industry, I also enjoy exploring new technologies and working on side projects in my own time‚Äîit‚Äôs a common way for developers to learn and grow outside of their day-to-day responsibilities.","background#Background":"During our migration from on-premise infrastructure to Azure, I had the opportunity to revisit and redesign our CI/CD pipelines. My guiding principle throughout this process was to keep things as simple as possible, always returning to first principles when evaluating tooling and process changes. GitHub Actions emerged as a strong candidate for our needs, offering flexibility and ease of use.","benefits-of-workflow-starters#Benefits of Workflow Starters":"Workflow starters reduce the need for extensive documentation or step-by-step guides, especially when migrating many applications. By providing ready-to-use templates, you lower the barrier to entry, encourage best practices, and help teams focus on delivering value rather than wrestling with configuration.\n1 The Enterprise plan allows you to create private .github repositories, which is useful for referencing secrets securely. If you‚Äôre not on the Enterprise plan, consider documenting how to handle secrets in your starter templates, using placeholders that can be replaced as needed.\n2 My preference is to keep documentation close to the code, using READMEs and markdown files within the repository to support onboarding and knowledge sharing.","creating-a-github-actions-workflow-starter#Creating a GitHub Actions Workflow Starter":"GitHub offers excellent documentation for creating workflow starters. You can find a helpful guide here: create a starter\nThis screenshot is demonstrative of the simplicity of what little you need to do to have a starter to share across your organisation:\nTo create a workflow starter, set up a public repository named .github with a workflow-templates folder. Place your starter Action YAML files there‚Äîeach can have an associated *.properties.json file to provide metadata and control when the starter is suggested. File patterns in the properties file allow you to offer relevant starters based on the contents of a repository.\nEssentially you‚Äôre creating a public repository named .github with a folder named workflow-templates. Contained within is your starter Action .yaml file. You can have many. Each Action file needs to have an associated *.properties.json file that labels and describes your starter. What adds to the depth of the relatively simple yet powerful feature is the ability to add intelligence to what starters are offered up to the author when creating an Action. This is made possible by the use of filepatterns. Anything that matches a file pattern in the root will predicate whether that starter is offered up or not. Clever hey? I‚Äôve included a sample *.properties.json for context below:\n{ \"name\": \"Deploy Workflow\", \"description\": \"Deploy to AKS\", \"iconName\": \"azure-icon\", \"categories\": [ \"csharp\" ], \"filePatterns\": [ \"package.json$\", \"^Dockerfile\", \".*\\\\.md$\" ] } When creating a new Action workflow, GitHub will suggest appropriate starters based on these patterns, making it easier for teams to get started quickly and consistently:","previous-experience#Previous Experience":"Before joining Carfinance 247, I worked with BitBucket Pipelines to build and deploy cloud-native solutions. I appreciated how this approach kept everything related to a solution in one place, reducing the need to learn or maintain additional systems. My early access to GitHub Actions further reinforced my preference for integrated, developer-friendly tooling.","tooling-landscape#Tooling Landscape":"At Carfinance 247, we have used a variety of CI/CD tools, including Bamboo, Azure DevOps, and GitHub Actions. Over time, we found that Bamboo was no longer meeting our needs due to reliability and performance issues. Azure DevOps, while powerful, introduced additional complexity with service connections, permissions, and configuration overhead. GitHub Actions, on the other hand, provided a more streamlined experience, even though some advanced features are only available on the Enterprise plan.\n‚ö†Ô∏è We have also learned that Azure DevOps is not on Microsoft‚Äôs long-term roadmap. "},"title":"Github Action Workflow Starter"},"/blog/github-actions-workflow-env-vars/":{"data":{"cicd#CICD":"Here‚Äôs a note on being sympathetic to our development teams‚Äô nuances‚Ä¶\n‚ÑπÔ∏è We are using the approach of regenerating feature images and deploying from one workflow_dispatcher instead of triggering a deployment from a merge to a dedicated development branch. Our (the virtual team I‚Äôm managing that is the Platform Team - senior staff) combined experience lead us to determine that a dedicated development branch will get out of sync with our main branch. This is especially problematic if your branching strategy predicates promoting to production via a merge to main from a dedicated development branch. To compound this point, we often have multiple developers concurrently working on the same repo so this in itself presents inherent complexities so arriving at a CICD pipelines wasn‚Äôt clear cut as teams have subtle nuances around how they build \u0026 deploy features/fixes. ","github-workflows#Github workflows":"In my current role as Head of Cloud Platform, I am leading the technical effort of migrating our entire on-premise real-estate to Azure. Part of this mission, is to upgrade the runtimes of our applications, regardless of their current placement; IIS Web apps, Windows Services and Docker Swarm containers. I say ‚Äúpart of this mission‚Äù as another aspect of this migration is to create a new foundation for our platform - AKS. I hope to cover more on this in later posts.\nGithub workflowsWe are using Self-Hosted Runners to build and deploy our applications to AKS. We have a Hub\u0026Spoke network architecture and our AKS clusters are private. We have other backing services that are deliberately behind Azure Private Endpoints. Our architecture enables us to deploy securely from our company network to our spoke VNETs that exist across our Azure Subscriptions.\nWe‚Äôre targeting 2 guest operating systems with our containerization orchestration solution - AKS. These are Linux (.NET Core workloads) and Windows (.NET Framework workloads). We are having to upgrade our .NET Framework runtimes to 4.8 as this is the minimum requirement to running containers in Kubernetes in Azure.\nThere are subtle GitHub Actions Workflows expression differences when working with Powershell and bash. Here in this post I concentrate on how you create and set env vars.\nIs the snippet below, which is from our deploy GHA workflow, I use a workflow_dispatch to deploy either a feature branch or our main branch. Feature branches are deployed to our Development Cluster and non-feature branches to our Production Cluster.\n- name: SETUP MAIN BRANCH if: ${{ github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' }} run: | echo \"ENV=prod\" \u003e\u003e $GITHUB_ENV echo \"TAG=1.0.${{github.run_number}}\" \u003e\u003e $GITHUB_ENV echo \"PWD=$(pwd)\" \u003e\u003e $GITHUB_ENV - name: SETUP FEATURE BRANCH if: ${{ github.ref != 'refs/heads/main' \u0026\u0026 github.ref != 'refs/heads/master' }} run: | echo \"ENV=dev\" \u003e\u003e $GITHUB_ENV echo \"TAG=${{ github.ref_name }}\" \u003e\u003e $GITHUB_ENV echo \"PWD=$(pwd)\" \u003e\u003e $GITHUB_ENV In the example above, we generate env vars that are used later in this workflow. The above is running on one of our Linux Self-Hosted Runners so using bash script.\nThe above example will not update env vars when run on a Windows Self-Hosted Runner (PowerShell).\nThe equivalent when targeting windows is:\n- name: SETUP MAIN BRANCH if: ${{ github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' }} run: | echo \"ENV=prod\" \u003e\u003e $env:GITHUB_ENV echo \"TAG=1.0.${{github.run_number}}\" \u003e\u003e $env:GITHUB_ENV echo \"PWD=$(Get-Location)\" \u003e\u003e $env:GITHUB_ENV - name: SETUP FEATURE BRANCH if: ${{ github.ref != 'refs/heads/main' \u0026\u0026 github.ref != 'refs/heads/master' }} run: | echo \"ENV=dev\" \u003e\u003e $env:GITHUB_ENV echo \"TAG=${{ github.ref_name }}\" \u003e\u003e $env:GITHUB_ENV echo \"PWD=$(Get-Location)\" \u003e\u003e $env:GITHUB_ENV Notationally, the only difference here is \u003e\u003e $GITHUB_ENV and \u003e\u003e $env:GITHUB_ENV. Powershell requires the pre-suffix of env: (as in Get-ChildItem env:). The consumer syntax of this env var remains the same between shells - ${{ env.TAG }} - so it‚Äôs only the creation of this env var that needs to change between shells.\nFor context, I‚Äôve pasted below an example of where the TAG env var is being consumed:\n- name: BUILD AND PUSH run: | try { cd ${{ env.ROOT_DIR }} docker build -t ${{ env.ACR_NAME }}/${{ env.APP_DOCKERIMAGE }}:${{ env.TAG }} -f ${{ env.ROOT_DIR }}/${{ env.APP_DOCKERFILE }} . docker push ${{ env.ACR_NAME }}/${{ env.APP_DOCKERIMAGE }}:${{ env.TAG }} } catch { Write-Output \"Could not push to ACR, error occured with docker build\" Exit 1 } "},"title":"Github Actions Workflow Env Vars"},"/blog/github-self-hosted-runner/":{"data":{"adding-more-github-self-hosted-runners#Adding more GitHub Self-Hosted Runners":"","bring-the-clonee-back-online#Bring the Clonee back online":"At this point, I could now longer see the Clonee‚Äôs hostname in the list of Runners.\nI returned to the Clonee VM and ran:\ncd actions-runners sudo ./svc.sh start This restarts the Runner as a service. It then became visible in the GitHub Runners page.","first-lets-deal-with-the-new-vm#First, let\u0026rsquo;s deal with the New VM":"Adding more GitHub Self-Hosted RunnersTo help build out our numbers of GitHub Self-Hosted Runner, we took a shortcut and had cloned an existing Linux VM.\nUnfortunately, the by-product of doing this resulted in (a) the clonee (source) Linux VM had their Self-Hosted hijacked by the new VM and (b) we had a Runner registered in GitHub that didn‚Äôt actually have a running runner - Offline ü§™.\nMadness!\nOk, so what to do?‚Ä¶\nThese are the steps I worked thru to rectify this:\nFirstly, remove Runner off of the new VM Next, rerun the config.sh step on new VM Finally, restart to Runner service on Clonee VM First, let‚Äôs deal with the New VM First of all, we need to remove the correct Runner service so I ran:\nsudo systemctl | grep runner ... sudo systemctl | grep runner actions.runner.\u003corg\u003e.\u003chostname\u003e.service loaded active I then copied the above service (actions.runner.\u003corg\u003e.\u003chostname\u003e.service) name into clipboard and past in below (\u003cservice-name\u003e)\nsystemctl stop \u003cservice-name\u003e systemctl disable \u003cservice-name\u003e Next, I returned to the GitHub portal and navigated to the Runners page and selected the new Runner. What I‚Äôm aiming to do here is to get a token that I can use to remove the Runner from the new VM:\nI pressed the Remove button\nthen copied and executed this on the new VM:\n./config.sh remove --token \u003credacted\u003e I re-ran the config.sh by:\n./config.sh --url https://github.com/\u003corg\u003e --token \u003credacted\u003e sudo ./svc.sh install sudo ./svc.sh start At this point I saw the Self-Hosted Runner return to the GitHub Runners page - Showing as Idle and not as Offline."},"title":"Adding more Github Self-Hosted Runners"},"/blog/how-to-conditionally-include-a-nuget-package/":{"data":{"the-context#The context":"The contextI have implemented a solution that as it‚Äôs primary objective iterates through a sequence of HTTP Requests and page interactions. With sequencing through these steps regularly it gives the support team the maximum amount of time to react to an outage. This outage can be isolated to this particular service, or can originate from any of it‚Äôs downstream dependencies.\nTo help with the automation of these steps, I‚Äôm using Selenium and in particular, it‚Äôs headless browser capability in conjunction with ChromeDriver. With this combination, I can navigate within a Chrome browser, provide alphanumerical inputs and execute mouse clicks, all by using imperative code.\nI am using serverless technologies to orchestrate it‚Äôs delivery, and for this solution‚Äôs resiliency. The particulars include an Azure FucntionsApp (Linux Container) that hosts the C# .NET 6.0 FunctionsApp, and the ChromeDriver which happens to be an external program (ergo, not managed code). I use the DevOps approach to build the FunctionsApp and associated unit tests, run the unit tests, produce code coverage stats (with min threshold), push the Docker image to ACR, deploy to a slot, assess error rates by calling az monitor log-analytics query ‚Ä¶ then performing a swap with the production slot post a manual verification step. These steps are incorporated into separate pipelines within Azure DevOps. There are multiple pipelines, one for infrastructure and one for the application. For the IaC I use the bicep DSL. I‚Äôm using common pipeline patterns such as variable and jobs templates. I‚Äôm also interacting with the az pipelines (azure-devops cli extension) to create/update variable-groups and variables.","the-issue#The issue":"During the execution of this FunctionsApp, I have observed warnings reporting a ChromeDriver mismatch with the Chrome browser, in the logs. This needed to be addressed.\nMy Dockerfile installs the latest stable version of Chrome - which happens to be version (major - semantic versioning) 105. However, my local development environment is constrained to using Chrome 104. To avoid any potential incompatibility issues, as well as to quieten these annoying warnings, I somehow need to implement a solution that requires zero maintenance.","the-solution#The solution":"Thanks to Mrs Google, and DDoSing this search engine with of terms such as nuget, conditions, linux, I eventually cobbled together a way to conditionally include package references. IDK there is a choose element within a csproj xml schema. I do now!\nI felt that the real challenge here was to include a package based on the intended operating system of the runtime. In dotnet vernacular, this is know as a Runtime Identifier. Examples of such are, linux-64, win-64, win-32. You provide this Runtime Identifier by using the -r (runtime) switch. A full example can be found later in this post.\nAdditionally, I felt it was important that the DX would not suffer consequently. So, I needed someway to set a default. This default will come into play when a developer runs this FunctionsApp via:\nthe dotnet run cli command or from within an IDE (Rider, VS) or code editor such as VSCode. In this example xml snippet, you can see how I‚Äôve implemented a default and how I used the Condition attribute to include packages based on the Runtime Identifier:\n\u003cChoose\u003e \u003cWhen Condition=\"$(RuntimeIdentifier) != ''\"\u003e \u003cItemGroup\u003e \u003cPackageReference Condition=\"$(RuntimeIdentifier.StartsWith('win'))\" Include=\"Selenium.WebDriver.ChromeDriver\" Version=\"104.0.5112.7900\" /\u003e \u003cPackageReference Condition=\"$(RuntimeIdentifier.StartsWith('linux'))\" Include=\"Selenium.WebDriver.ChromeDriver\" Version=\"105.0.5195.1900\" /\u003e \u003c/ItemGroup\u003e \u003c/When\u003e \u003cOtherwise\u003e \u003cItemGroup\u003e \u003cPackageReference Include=\"Selenium.WebDriver.ChromeDriver\" Version=\"104.0.5112.7900\" /\u003e \u003c/ItemGroup\u003e \u003c/Otherwise\u003e \u003c/Choose\u003e I specify the runtime identifier of linux-64 in the Dockerfile when I publish the .NET Core FunctionsApp. This can be seen in the command snippet below. This will ensure that the version (major) 105 is included:\ndotnet publish -p:PublishChromeDriver=true .\\my-functionsapp.csproj -c release --self-contained -r linux-x64 --output ./publish And if you set the verbosity of the output by adding this switch -v d (detailed) you will have visual confirmation that the appropriate version has been included:\nAnd there you have it, an example of how to include a different version of a nuget package."},"title":"How to Conditionally Include a Nuget Package"},"/blog/how-to-use-kubernetes-configmap/":{"data":{"":"There‚Äôs a ton of material out there on how to use a ConfigMap. In this post I will provide a recap on the basics then I drill into how to protect your secrets!\nThere are a few ways to create a configMap. Here, I cover just two of these ways;--from-env-file and ‚Äìfrom-literal. I won‚Äôt cover options like from volume.","how-to-create-a-configmap-from-a-literal#How to create a ConfigMap from a literal":"To create a configMap from literals and from the command line, you would type this:\n$ kubectl create configmap config-demo-lit --from-literal=user.name=garrardkitchen --from-literal=user.type=admin To confirm the values, you would type this:\nkubectl get cm config-demo-lit -o yaml apiVersion: v1 data: user.name: garrardkitchen user.type: admin kind: ConfigMap metadata: creationTimestamp: \"2020-11-02T16:06:30Z\" name: config-demo-lit namespace: dapr-demo resourceVersion: **** selfLink: /api/v1/namespaces/dapr-demo/configmaps/config-demo-lit uid: **** ","how-to-create-a-configmap-from-an-env-file#How to create a ConfigMap from an .env file":"From the command line To create a configMap from the command line, you would type this:\n$ kubectl create configmap demo-config --from-env-file=config/.env.prod To confirm the values, you would type this:\n$ kubectl cm config-demo-1 -o yaml apiVersion: v1 data: foo: baa name: garrard kind: ConfigMap metadata: creationTimestamp: \"2020-11-02T15:44:52Z\" name: config-demo-1 namespace: dapr-demo resourceVersion: **** selfLink: /api/v1/namespaces/dapr-demo/configmaps/config-demo-1 uid: **** üëÜ cm is shorthand for configmap\nFrom a Kubernetes Manifest file To create a configMap from a manifest, you would create a yml|yaml file using the kind: ConfigMap like this:\napiVersion: v1 kind: ConfigMap metadata: name: config-demo-2 namespace: dapr-demo data: foo: baa name: garrard To confirm the values, you would type this:\n$ kubectl cm config-demo-2 -o yaml apiVersion: v1 data: foo: baa name: garrard kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"data\":{\"foo\":\"baa\",\"name\":\"garrard\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"name\":\"config-demo-2\",\"namespace\":\"dapr-demo\"}} creationTimestamp: \"2020-11-02T15:49:48Z\" name: config-demo-2 namespace: dapr-demo resourceVersion: ***** selfLink: /api/v1/namespaces/dapr-demo/configmaps/config-demo-2 uid: **** ","how-to-stop-people-from-finding-out-your-secrets#How to stop people from finding out your secrets.":"At the end of the day, the secrets are only Base64 encoded. Anyone with the appropriate level of permissions will be able to see your secrets. One way to stop users from seeing your secrets is by only allow particular groups of people access.\nTo create a secret, type:\n$ kubectl create secret generic db-passwords --from-literal=mongodb-password='mypassword' To see what secrets we have, type:\n$ kubectl get secrets NAME TYPE DATA AGE dapr-operator-token-mgdqs kubernetes.io/service-account-token 3 2d13h dapr-sidecar-injector-cert Opaque 2 2d13h dapr-trust-bundle Opaque 3 2d13h dashboard-reader-token-j9rcg kubernetes.io/service-account-token 3 2d13h db-passwords Opaque 1 5s default-token-xl2rz kubernetes.io/service-account-token 3 2d14h sh.helm.release.v1.dapr.v1 helm.sh/release.v1 1 2d13h To see the actual password, type:\n$ kubectl get secrets db-passwords -o yaml apiVersion: v1 data: mongodb-password: bXlwYXNzd29yZA== kind: Secret metadata: creationTimestamp: \"2020-11-03T10:00:14Z\" name: db-passwords namespace: dapr-demo resourceVersion: **** selfLink: /api/v1/namespaces/dapr-demo/secrets/db-passwords uid: **** üö´ This is just a Base64 encoded string of mypassword. This is not secure enough. We need another way of to protect our sensitive information/passwords. So, what do we do?\nHere‚Äôs a link to how Kubernetes deals with secrets\nTo use this secret with a deployment, save this to aks-deploy-mongodb-demo.yml:\nPlease note, this is not a production configuration\napiVersion: v1 kind: Service metadata: name: mongodb-svc namespace: dapr-demo labels: run: mongodb-svc spec: type: LoadBalancer ports: - port: 27017 targetPort: 27017 protocol: TCP selector: run: mongodb --- apiVersion: apps/v1 # for k8s versions before 1.9.0 use apps/v1beta2 and before 1.8.0 use extensions/v1beta1 kind: Deployment metadata: name: mongodb namespace: dapr-demo spec: selector: matchLabels: run: mongodb replicas: 1 template: metadata: labels: run: mongodb spec: containers: - name: mongodb image: mongo resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 27017 env: - name: MONGO_USERNAME valueFrom: configMapKeyRef: name: config-demo-lit key: user.name - name: MONGO_INITDB_ROOT_USERNAME valueFrom: configMapKeyRef: name: config-demo-lit key: user.name - name: MONGO_INITDB_ROOT_PASSWORD valueFrom: secretKeyRef: name: db-passwords key: mongodb-password - name: MONGO_DBNAME value: \"orders\" To deploy the above üëÜ, type this:\n$ kubectl apply -f .\\aks-deploy-mongodb-demo.yml ","how-to-use-secrets#How to use Secrets":"TBC","how-to-use-this-in-a-pod#How to use this in a pod":"Here, I‚Äôm setting up environment variables from different ConfigMaps. config-demo-2 is set up from manifest file and config-demo-lit is set up from literals.\nThis is an example pod manifest called pod-demo.yml\napiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: test-cache image: k8s.gcr.io/busybox command: [\"/bin/sh\", \"-c\", \"env\"] env: - name: NAME valueFrom: configMapKeyRef: name: config-demo-2 key: name - name: ROLE valueFrom: configMapKeyRef: name: config-demo-lit key: user.type restartPolicy: Never All that this üëÜ does, is output to STDOUT, a list of environment variables.\nTo apply this manifest, type:\nkubectl.exe apply -f .\\pod-demo.yml To confirm the 2 environment variables have been set, type:\n$ kubectl logs test-pod ... HOSTNAME=test-pod NAME=garrard ROLE=admin ... "},"title":"How to Use Kubernetes Configmap"},"/blog/how-to-write-clipboard-to-file/":{"data":{"":"Have you ever wondered how to persist your clipboard to disk? Wonder no more‚Ä¶\nTo get started, copy a random sentence into your clipboard like so:\nSet-Clipboard -Value \"This is being copied into clipboard\" Let‚Äôs confirm the contents of the clipboard, like so:\nGet-Clipboard output:\nThis is being copied into clipboard We‚Äôll now persist the same cliboard to a file, like so:\nGet-Clipboard | Out-File -FilePath clipboard.txt To confirm the contents of this file, we type:\nGet-Content .\\clipboard.txt output:\nThis is being copied into clipboard Let‚Äôs append something else into the clipboard like so:\nSet-Clipboard \"appended!\" -Append And finally, we confirm that this text has indeed been copied into the clipboard like so:\nGet-Clipboard output:\nThis is being copied into clipboard appended! There you have it, a short post on how to obtain and set the clipboard using Powershell commands.\nModule: Microsoft.PowerShell.Management"},"title":"How to Write Clipboard to File"},"/blog/hugo-shortcodes/":{"data":{"#":"My first attemptHere‚Äôs my first effort at creating a shortcode.\nThis shortcode is available here\nInformation Basic {{\u003c note Sample text \u003e}}\nSample text With italics {{\u003c note italic=‚Äútrue‚Äù Sample text \u003e}}\nSample text With header {{\u003c note title=‚ÄúWith header‚Äù\u003e Sample text \u003e}}\nWith header Sample text Warning Basic {{\u003c note warning=‚Äútrue‚Äù\u003e Sample text \u003e}}\nSample text With italic {{\u003c note warning=‚Äútrue‚Äù italic=‚Äútrue‚Äù Sample text \u003e}}\nSample text With header {{\u003c note warning=‚Äútrue‚Äù title=‚ÄúWith header‚Äù Sample text \u003e}}\nWith header Sample text Error Basic {{\u003c note error=‚Äútrue‚Äù Sample text \u003e}}\nSample text With italic {{\u003c note error=‚Äútrue‚Äù italic=‚Äútrue‚Äù Sample text \u003e}}\nSample text With header {{\u003c note error=‚Äútrue‚Äù title=‚ÄúWith header‚Äù Sample text \u003e}}\nWith header Sample text ","my-first-attempt#My first attempt":""},"title":"Hugo Shortcodes - my first try"},"/blog/hybrid-origins-http-traffic/":{"data":{"":"We‚Äôre migrating our on-premise workloads to Azure. This has presented several challenges. One of which is what I am covering specifically here in this post and that is ‚Ä¶\nHow to reduce code change effort?\nThis isn‚Äôt about updating runtimes, this is about having workloads spread across different platforms that need to talk to each other (with some HTTP chaining üëÄ). It is not uncommon for one HTTP API to need to talk to another HTTP API. If you move one HTTP service to run in different zone, you will need update references in those ALL dependant services so they point to that new DNS Hostname. This isn‚Äôt a big issue if you‚Äôve only few HTTP APIs. However, if you‚Äôve 100+ HTTP APIs that you‚Äôre migrating, including many with several HTTP dependencies, then this quickly becomes daunting and a potential PR approval and deployment (multiple environments) scheduling nightmare. A simple domain name search in your organisation‚Äôs Github account will illuminate my point.\nOk, so the nightmare scenario has been painted. What can you do.\nWith the HttpClients we‚Äôre using - .NET Framework and .NET Core, as a default have AllowAutoRedirect as true. So why is this important. Well, if you want to simply have a redirect rule return a temporary redirect - 302 - then your HttpClient will automatically react to this and reissue the same HTTP Request. However, it does not use the original Authorization header. Yikes.\nWe use CloudFlare. They are the best at what they do. We‚Äôre looking; as company we have an excellent relationship with CF and with that we get Stirling advice.\nWe originally used their Page Rule redirect to map to a different host. This is where we observed that the authz header being stripped on the AutoRedirect.\nOk, so what can be do? Well, we can make lots of additional code changes to plug this use-case in all HTTP Handler related code and be hated by all engineers for the rest of my days or look for a cross-cutting solution. Back to CF we go‚Ä¶ . Two of my guiding principles for the migration effort is (1) to reduce cognitive overload and (2) effort required (this includes unnecessary code changes). Ultimately, simplify all facets of the migration. I don‚Äôt want my engineers feeling the same levels of pain that I am.\nI can confirm we have a solution and it is in play. Boo-yah!\nThere are few things you have to do. Here‚Äôs a short list that I will go into more detail on shortly:\nCF Page rules in domain you‚Äôre mapping from CF CNAME set as DNS proxy DNS forwarders if you‚Äôve dev/test environment on-premise If you‚Äôre using Windows VMs with IIS to server up HTTP APIs, you need to a new hostname to your bindings list nginx origin configs (to receive and forward requests from new hostname so you can have weighted traffic load-balanced across both new and old environments during the rollout of to the new target of a particular HTTP API) Yes, there‚Äôs plenty there, most of which can be automated - think Terraform.\nI‚Äôm going to focus on changes to CF here as this is where the real magic happens!"},"title":"Hybrid Origins Http Traffic"},"/blog/inner-loop/":{"data":{"":"This post explores the concept of the Inner Loop in software development, emphasizing its importance in enhancing productivity, reducing feedback delays, and improving code quality. It provides practical examples and tools to optimize the inner loop, ensuring a more efficient and enjoyable development process.","challenges-and-solutions#Challenges and Solutions":"Optimizing the inner loop is not without its challenges. Developers often face issues such as slow build times, lack of proper tooling, or resistance to adopting new practices. Here are some solutions:\nSlow Build Times: Use incremental builds and caching mechanisms to speed up the process. Tools like Webpack or Bazel can help optimize build times. Lack of Proper Tooling: Invest in modern IDEs and plugins that support features like real-time code analysis and debugging. Resistance to Change: Provide training and demonstrate the benefits of inner loop optimization to encourage adoption. ","examples-of-inner-loop-optimization#Examples of Inner Loop Optimization":" Hot Module Replacement (HMR): Frameworks like React and Vue.js support HMR, allowing developers to see changes in real-time without refreshing the entire application. Integrated Debugging: Modern IDEs like Visual Studio Code and JetBrains Rider provide integrated debugging tools, enabling developers to identify and fix issues quickly. Local Test Execution: Running tests locally using tools like pytest for Python or Mocha for JavaScript ensures that code changes do not break existing functionality. Pre-commit Hooks: Tools like Husky can enforce code quality checks and run tests before code is committed, preventing issues from entering the version control system. .NET Aspire: This tool is specifically designed to enhance the inner loop for .NET developers. By integrating features like real-time code analysis, dependency management, and performance profiling, .NET Aspire helps developers identify and resolve issues early in the development process. For example, its built-in analyzers can highlight potential performance bottlenecks or security vulnerabilities as you write code, ensuring that your application is both efficient and secure. Additionally, .NET Aspire‚Äôs seamless integration with popular IDEs like Visual Studio makes it an invaluable asset for streamlining the inner loop. By implementing these practices and tools, teams can create a more efficient and enjoyable development experience, ultimately leading to better software and happier developers.","future-trends#Future Trends":"Emerging technologies are set to revolutionize the inner loop. For example:\nAI-Assisted Development: Tools like GitHub Copilot can suggest code snippets and automate repetitive tasks, further streamlining the inner loop. Cloud-Based Development Environments: Platforms like GitHub Codespaces allow developers to work in pre-configured environments, reducing setup time and ensuring consistency across teams. ","how-can-you-provide-more-feedback#How can you provide more feedback?":"As alluded to at the end of the opening section, there are more ways you can provide feedback:\nSAST (Static Application Security Testing): Tools like Snyk or Checkmarx can identify security vulnerabilities in your code during the inner loop, allowing you to address them before they become critical. Analyzers: Linters and code analyzers, such as ESLint for JavaScript or Roslyn analyzers for .NET, can enforce coding standards and highlight potential issues in real-time. Standards: Adopting and adhering to coding standards ensures consistency and maintainability across the codebase. For example, using a style guide like PEP 8 for Python or the Google Java Style Guide can help maintain high-quality code. CI Feature Pipeline: Setting up a lightweight CI pipeline that runs essential tests and checks during the inner loop can provide immediate feedback. For instance, GitHub Actions can be configured to run unit tests and linters on every pull request, ensuring that only high-quality code is merged. ","metrics-for-success#Metrics for Success":"To measure the effectiveness of inner loop optimization, consider tracking the following metrics:\nBuild and Test Times: Monitor how long it takes to build and test code locally. Bug Detection Rate: Track the number of bugs caught during the inner loop versus later stages. Developer Satisfaction: Conduct surveys to gauge how developers feel about their workflow and tools. ","sustainability-and-reduced-feedback-delays#Sustainability and Reduced Feedback Delays":"Optimizing the inner loop also contributes to sustainability in software development. By reducing the time and resources spent on fixing issues in later stages, teams can focus on delivering value to users. For example, addressing performance bottlenecks during development can lead to more efficient applications, reducing energy consumption and operational costs.\nMoreover, reducing feedback delays is crucial for maintaining momentum and avoiding context-switching. When developers receive immediate feedback, they can address issues while the context is still fresh in their minds. This not only improves the quality of the code but also accelerates the development process, enabling teams to meet deadlines and deliver features faster.","what-is-the-inner-loop#What is the Inner-loop?":"The ‚ÄúInner Loop‚Äù in software development refers to the set of activities and processes a developer repeatedly goes through while writing, testing, and debugging code in the development phase, before committing changes to a version control system. This loop typically includes writing code, compiling, running, testing locally, and debugging. The goal of optimizing the inner loop is to maximize developer productivity and satisfaction by making these activities as efficient and frictionless as possible.\nAs described by Gene Kim in his book called, ‚ÄúDevOps Handbook‚Äù, the 2nd Way, you optimize for fast feedback (from right to left). Building and testing locally, does contribute towards this 2nd way but we can do more, and often do, to provide additional feedback. I will expand on this shortly.","why-is-this-important#Why is this important?":"I‚Äôve summarized the importance of the inner loop here:\nMaximizing inner-loop time boosts productivity and personal satisfaction for developers. Fix bugs before integrating your changes into the mainline branch. Reducing outer-loop friction (through better tooling and automation) to minimize disruptions. Improved Code Quality Faster Time to Market Enhanced Collaboration Reduced Context Switching I will now expand on the above list:\nMaximizing inner-loop time boosts productivity and personal satisfaction for developers The inner loop is where developers spend most of their time. By optimizing this phase, developers can focus on creative problem-solving and writing high-quality code without unnecessary interruptions. For example, using tools like hot-reloading in frameworks such as React or .NET can significantly reduce the time spent waiting for changes to reflect during development. This not only enhances productivity but also contributes to job satisfaction, as developers can see the immediate impact of their work.\nFix bugs before integrating your changes into the mainline branch. Catching and fixing bugs early in the inner loop prevents them from propagating to later stages of development, where they become more costly and time-consuming to address. For instance, running unit tests locally before committing code ensures that basic functionality is intact. Tools like Jest for JavaScript or NUnit for .NET can automate this process, providing instant feedback and reducing the risk of introducing defects into the mainline branch.\nReducing outer-loop friction Outer-loop activities, such as code reviews and CI/CD pipeline executions, often introduce delays. By addressing issues in the inner loop, developers can minimize the need for rework and streamline the transition to the outer loop. For example, using static code analysis tools like SonarQube or Qodana during development can catch code quality issues early, reducing the burden on code reviewers and speeding up the overall development process.\nImproved Code Quality By focusing on the inner loop, developers can ensure that their code meets high standards before it is shared with the team. This includes adhering to best practices, following coding standards, and using tools like linters and formatters to maintain consistency. High-quality code is easier to maintain, extend, and debug, reducing technical debt in the long run.\nFaster Time to Market Optimizing the inner loop accelerates the development process, enabling teams to deliver features and fixes more quickly. This is particularly important in competitive industries where time to market can be a critical factor in success. By reducing delays and inefficiencies, teams can respond to user needs and market demands more effectively.\nEnhanced Collaboration When developers address issues in the inner loop, they reduce the burden on code reviewers and other team members. This fosters a more collaborative environment, as team members can focus on higher-level concerns rather than fixing basic issues. It also improves the overall workflow, making it easier for teams to work together efficiently.\nReduced Context Switching Immediate feedback in the inner loop helps developers stay focused on their tasks. When issues are identified and resolved quickly, developers can avoid the cognitive load of switching between different tasks or revisiting code they wrote days or weeks earlier. This leads to a more seamless and productive development experience."},"title":"What is the Inner Loop?"},"/blog/jest-fs-readfilesync/":{"data":{"":"I‚Äôd just ran npm run test in a newly created package I‚Äôd added to a monorepo (lerna) I‚Äôd created for a project I was working on that integrates with Twilio Sync, RabbitMQ, Twilio TaskRouter and MSSQL, and I go this:\n*******************************consumers\\packages\\eda [CRMBROK-233 +0 ~2 -0 !]\u003e npm run test \u003e @cf247/eda@1.0.2 test *******************************consumers\\packages\\eda \u003e jest FAIL __tests__/eda.test.js ‚óè Test suite failed to run ENOENT: no such file or directory, open '.env' 2 | const fs = require('fs') 3 | const dotenv = require('dotenv') \u003e 4 | const envConfig = dotenv.parse(fs.readFileSync(`.env`)) | ^ 5 | for (const k in envConfig) { 6 | process.env[k] = envConfig[k] 7 | } at Object.\u003canonymous\u003e (lib/setenv.js:4:35) at Object.\u003canonymous\u003e (lib/eda.js:1:1) Test Suites: 1 failed, 1 total Tests: 0 total Snapshots: 0 total Time: 1.772 s Ran all test suites. npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! @cf247/eda@1.0.2 test: `jest` npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the @cf247/eda@1.0.2 test script. npm ERR! This is probably not a problem with npm. There is likely additional logging output above. npm WARN Local package.json exists, but node_modules missing, did you mean to install? npm ERR! A complete log of this run can be found in: npm ERR! *******************************\\npm-cache\\_logs\\2020-05-28T08_04_32_271Z-debug.log *******************************consumers\\packages\\eda [CRMBROK-233 +0 ~3 -0 !]\u003e Not great but hey, first run and all!\nThe error message tells me everything I need to know:\nENOENT: no such file or directory, open '.env' 2 | const fs = require('fs') 3 | const dotenv = require('dotenv') \u003e 4 | const envConfig = dotenv.parse(fs.readFileSync(`.env`)) Which is that it can‚Äôt find an .env file. And it wouldn‚Äôt. Later refactoring would remove this file dependency but for now, all I want to do is to get my test working.\nThis was the unit test code:\n'use strict' const eda = require('..') describe('@cf247/eda', () =\u003e { it('no tests', () =\u003e { }) }) This is the code from the module it was importing via the require('..') statement:\nrequire('./setenv') const amqp = require('amqplib/callback_api'); module.exports = (io, emitter) =\u003e { ... The top line is importing code from this file:\nI‚Äôve highlighted the problematic line of code\n1 2 3 4 5 6 const fs = require('fs') const dotenv = require('dotenv') const envConfig = dotenv.parse(fs.readFileSync(`.env`)) for (const k in envConfig) { process.env[k] = envConfig[k] } The quickest (IMO) way to deal with this and move forward is to Mock the fs class. I did this by included a jest module mock into my unit test file:\nI‚Äôve highlighted the mock related code\n1 2 3 4 5 6 7 8 9 10 11 12 13 'use strict' const fs = require('fs') const eda = require('..') jest.mock('fs', () =\u003e ({ readFileSync: jest.fn((file_name) =\u003e { return [] }) })) describe('@cf247/eda', () =\u003e { it('no tests', () =\u003e { }) }); What this does is, when the readFileSync class function is called, it always returns an empty array []. As the unit code does not have a dependency on environment variables, this mocked response will work fine."},"title":"Unit testing and mocking fs.ReadFileSync"},"/blog/k8s-pdb/":{"data":{"how-to-ensure-theres-at-least-one-pod-running#How to ensure there\u0026rsquo;s at least one pod running":"Pod Disruption BudgetWhen working with Kubernetes, one crucial component of configuration is known as a PDB (Pod Disruption Budget). A PDB will ensure your workload remains running when you work through a Voluntary Disruption.\nWhat on earth is a Voluntary Disruption? A Voluntary Disruption is when you trigger an action that causes the disruption. For example, if you wish to upgrade a Minor AKS version or any action that recycles a Node Pool. Click here ‚û° Disruptions to read up on what Disruptions are.\nThis is what a PDB manifest looks like this. This example tells Kubernetes to make sure there‚Äôs always a minimum of 2 Pods running during a disruption:\napiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: my-awesome-microservice-pdb spec: minAvailable: 2 selector: matchLabels: app: my-awesome-microservice-api We use Helm Charts so part the declaration that wraps the minAvailable property looks like this:\n{{- if hasKey .Values \"pdb\" }} {{- if hasKey .Values.pdb \"minAvailable\" }} minAvailable: {{ .Values.pdb.minAvailable }} {{- end }} {{- else }} minAvailable: {{ max (sub .Values.replicaCount 1) 1 }} {{- end }} What on earth is going on here then?!\nThere are a few rules that I need to accommodate for within our workload PDBs. These are:\nApply a specific value that may be contained within a values .yaml file Provide a default value if one is not supplied Ensure there‚Äôs at least 1 pod running throughout the disruption so a workload doesn‚Äôt go offline during this period. üò± How to use the value provided by the developerI‚Äôve designed our CICD pipeline so we get base configurations (one .NET Framework IIS workloads, one for .NET Core Web workloads, one for ‚Ä¶ etc.) from one git repo, and get all application(s) configuration properties from the application git repo itself (üìÇ /.k8s/). It is here from within the application‚Äôs repo we set the properties for a service(s) within a values-\u003cenv\u003e.yaml file. If there‚Äôs more than one application found in an application‚Äôs git repo, the name is reflected in the name of values .yaml file to provide uniqueness - eg values-\u003cconsumer\u003e-\u003cenv\u003e.yaml.\nIt is in this values .yaml file we set - if at all - a value to the pdb.minAvailable nested property.\nHere, in this control flow, we are checking that both pdb and minAvailable properties exist. If they do, we apply the minAvailable value. We use the hasKey function to good effect to check for the existence of a property in another property:\n{{- if hasKey .Values \"pdb\" }} {{- if hasKey .Values.pdb \"minAvailable\" }} minAvailable: {{ .Values.pdb.minAvailable }} {{- end }} {{- else }} How to provide a default valueIf the application configuration does not contain a minAvailable property, we take the value found in the replicaCount value and use this. However, we do not insist on the same value, but instead 1 less - (sub .Values.replicaCount 1).\n... {{- else }} minAvailable: {{ max (sub .Values.replicaCount 1) 1 }} {{- end }} How to ensure there‚Äôs at least one pod runningWe must ensure at least one instance of a workload is running and if we simply reduced the replicaCount by one and left it at that, we could end up with a budget of zero. I don‚Äôt want this to happen. What I do here is use the max function to good effect to safeguard against this ever being a zero - and if replicaCount: 1, then the expression would read max (0) 1, meaning 1 would be the value used.\n... {{- else }} minAvailable: {{ max (sub .Values.replicaCount 1) 1 }} {{- end }} ","how-to-provide-a-default-value#How to provide a default value":"","how-to-use-the-value-provided-by-the-developer#How to use the value provided by the developer":"","pod-disruption-budget#Pod Disruption Budget":"","references#References":" Helm Functions Flow control Disruptions Best practices - Voluntary Disruption "},"title":"Kubernetes Pod Disruption Budget and the Helm hasKey Function"},"/blog/k8s-selectors-and-labels/":{"data":{"binding-deployment-to-pod#Binding deployment to pod":"So, how do we couple the Deployment with the Pod? Well, this is where the selector comes into play. The selector instructs Kubernetes to match on the app label for those that have a value of nginx and that the foo label that has the value of baa.\nspec: selector: matchLabels: app: nginx foo: baa I hope this has made sense and has cleared up any confusion you may have had.","the-first-metadata-reference#The first metadata reference":"Right, what‚Äôs the deal with all the labels and metadata in a Deployment manifest?!!!!\nTake this example:\napiVersion: apps/v1 kind: Deployment metadata: namespace: default name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx foo: baa template: metadata: labels: app: nginx foo: baa spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 Here, we see metadata twice, and also there‚Äôs mention of matchLabels in selector??? What does it all mean???\nOk, let me explain üëÄ ‚Ä¶\nThe first metadata referenceA deployment manifest kind is a manifest that describes the desired state of your application(s). I say applications here as a POD can contain more than one container (application). The desired part of this is found in a ReplicaSet kind manifest. For example, you‚Äôd use a ReplicaSet if you require to have 2 replicas (instances) of your POD running. A Deployment manifest is a short-hand way of stipulating this, ergo, saves you having to create 2 separate manifests. Makes sense? Good.\n‚ÑπÔ∏è Behind the scenes, it is the Deployment Controller that monitors your deployment‚Äôs desired state and if it differs, it will return to it‚Äôs desired state. So why is there two mentions of metadata? Ok, The first reference identifies this Deployment object itself:\napiVersion: apps/v1 kind: Deployment metadata: namespace: default name: nginx-deployment labels: app: nginx So for example, if you want to delete this object, you‚Äôd issue either of these kubectl commands:\nkubectl delete deployments nginx-deployment -n default OR kubectl delete deployments -l app=nginx-deployment -n default The latter delete example above uses an equality-based label selector condition. There‚Äôs also a set-based label selector condition. You can read about these here ‚û° labels and selectors","the-second-metadata-reference#The second metadata reference":"The second metadata reference is inside the template that is being used to describe the POD to be created AND is separate from the Deployment itself.\n... template: metadata: labels: app: nginx foo: baa spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 To demonstrate this, let‚Äôs run the following command:\n‚ùØ kubectl.exe get pods -l app=nginx -n default --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-6494589cc9-242v8 1/1 Running 0 3s app=nginx,foo=baa,pod-template-hash=6494589cc9 nginx-deployment-6494589cc9-2fdf5 1/1 Running 0 3s app=nginx,foo=baa,pod-template-hash=6494589cc9 nginx-deployment-6494589cc9-n8vxb 0/1 ErrImagePull 0 3s app=nginx,foo=baa,pod-template-hash=6494589cc9 If you look at the LABELS column you can see labels that are not found in the Deployment metadata - eg foo=baa.\nYou can also use set-based selector; here‚Äôs an example that produces the same outcome:\n‚ùØ kubectl.exe get pods -l \"app in (nginx)\" -n default --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-6494589cc9-6sn7v 1/1 Running 0 6m10s app=nginx,foo=baa,pod-template-hash=6494589cc9 nginx-deployment-6494589cc9-htxpx 0/1 ImagePullBackOff 0 6m10s app=nginx,foo=baa,pod-template-hash=6494589cc9 nginx-deployment-6494589cc9-qh8nc 0/1 ErrImagePull 0 6m10s app=nginx,foo=baa,pod-template-hash=6494589cc9 "},"title":"K8s Selectors and Labels"},"/blog/kubernetes-on-windows/":{"data":{"":"","#":"This post is a reminder to me of what needs to be installed in order for a pod, created from a local image, that is to be served up via a kubernetes cluster, to be run from your local development environment.\nWhat is Kubernetes and why is it so important? ‚ÄúKubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.‚Äù\nSo why is Kubernetes important? ‚ÄúContainers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn‚Äôt it be easier if this behavior was handled by a system?\nThat‚Äôs how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example, Kubernetes can easily manage a canary deployment for your system.‚Äù\nKubernetes is the community‚Äôs (has a much larger community than that of Swarm's community) choice of container orchestrators.\nSome important notices ‚ÑπÔ∏è Permissions\nTo install kubectl and minikube you must start Powershell with Administrator permissions\n‚ö†Ô∏è Shell\nThese settings will only viable for the current shell, if you need to run another shell, ensure the minikube docker-env commands in the Steps to take to configure your environment section are also executed in the new shell. As minikube is the tool that runs a local cluster in your development environment, we need to tell it to use it‚Äôs built-in docker daemon and have images pulled from there, and not from a container registry.\nHow do I install kubectl (and what the heck is it)? kubectl is a CLI (command line interface) tool for controlling Kubernetes clusters. You can use this tool to deploy applications, inspect and manage cluster resources and view logs.\nTo ease the installation process, use chocolatey to install kubernetes-cli, run:\nPS C:\\\u003e choco install kubernetes-cli How do I install minikube (and what the heck is it)? Minikube implements a local Kubernetes cluster and is deemed the best tool for local Kubernetes application development.\nTo ease the installation process, use chocolatey to install minikube, run:\nPS C:\\\u003e choco install minikube Before you start, you must ensure that you have a platform virtualisation system available. Platform virtualisation software provides the mechanism to run virtual machines and containers in isolation and exposes them to one or more networks. It is within a virtual machine that your Kubernetes cluster will run. Windows 10 comes with a virtualisation hypervisor feature called hyper-v. You need to ensure it is running first. To do this, run:\nPS C:\\\u003e Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All PS C:\\\u003e minikube start --driver hyperv * minikube v1.9.1 on Microsoft Windows 10 Enterprise 10.0.18363 Build 18363 * Using the hyperv driver based on user configuration * Downloading VM boot image ... \u003e minikube-v1.9.0.iso.sha256: 65 B / 65 B [--------------] 100.00% ? p/s 0s \u003e minikube-v1.9.0.iso: 174.93 MiB / 174.93 MiB [ 100.00% 1.03 MiB p/s 2m51s * Starting control plane node m01 in cluster minikube * Creating hyperv VM (CPUs=2, Memory=6000MB, Disk=20000MB) ... * Preparing Kubernetes v1.18.0 on Docker 19.03.8 ... * Enabling addons: default-storageclass, storage-provisioner * Done! kubectl is now configured to use \"minikube\" Steps to take to configure your environment To set up your minikube environment, run:\nPS C:\\\u003e minikube docker-env $Env:DOCKER_TLS_VERIFY = \"1\" $Env:DOCKER_HOST = \"tcp://192.168.75.126:2376\" $Env:DOCKER_CERT_PATH = \"C:\\Users\\garrard.kitchen\\.minikube\\certs\" $Env:MINIKUBE_ACTIVE_DOCKERD = \"minikube\" # To point your shell to minikube's docker-daemon, run: # \u0026 minikube -p minikube docker-env | Invoke-Expression To point your shell to minikube‚Äôs docker-daemon, run:\nPS C:\\\u003e minikube docker-env | Invoke-Expression To get access to minikube‚Äôs dashboard, run:\nPS C:\\\u003e minikube.exe dashboard * Verifying dashboard health ... * Launching proxy ... * Verifying proxy health ... * Opening http://127.0.0.1:54553/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser... Here‚Äôs some sample nodejs (server.js) code. It starts a server on port 8080:\nserver.jsvar http = require('http'); var handleRequest = function (request, response) { console.log('Received request for URL: ' + request.url); response.writeHead(200); response.end('Hello World!'); }; console.log(\"started\") var www = http.createServer(handleRequest); www.listen(8080); Here‚Äôs a Dockerfile for the above nodejs server. Please observe that it exposes port 8080. This ensures that network TCP traffic can be received by the container via port 8080.\ndockerfileFROM node:13.5.0 EXPOSE 8080 COPY server.js . CMD [ \"node\", \"server.js\" ] To build a image of the above Dockerfile, run:\nPS C:\\\u003e docker build -t hello-world:1 . ‚ö†Ô∏è Include a build tag\nYou must specify a version tag and it has to be something other than latest. Here, I have used 1. If you don‚Äôt follow these instructions, minikube will attempt to pull the image from a docker registry (normally DockerHub).\nTo check that the image exists in Minikube‚Äôs built-in Docker daemon, run:\nPS C:\\\u003e minikube ssh $ docker images You should see something similar to this:\n$ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , \u003c ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world 1 55f40b7f5c32 13 days ago 660MB hello-world latest 50c4285f25a5 13 days ago 660MB nginx latest ed21b7a8aee9 2 weeks ago 127MB k8s.gcr.io/kube-proxy v1.18.0 43940c34f24f 3 weeks ago 117MB k8s.gcr.io/kube-scheduler v1.18.0 a31f78c7c8ce 3 weeks ago 95.3MB k8s.gcr.io/kube-apiserver v1.18.0 74060cea7f70 3 weeks ago 173MB k8s.gcr.io/kube-controller-manager v1.18.0 d3e55153f52f 3 weeks ago 162MB kubernetesui/dashboard v2.0.0-rc6 cdc71b5a8a0e 5 weeks ago 221MB k8s.gcr.io/pause 3.2 80d28bedfe5d 2 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 2 months ago 43.8MB kindest/kindnetd 0.5.3 aa67fec7d7ef 5 months ago 78.5MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 5 months ago 288MB kubernetesui/metrics-scraper v1.0.2 3b08661dc379 5 months ago 40.1MB gcr.io/k8s-minikube/storage-provisioner v1.8.1 4689081edb10 2 years ago 80.8MB To run this image as a pod, run:\nPS C:\\\u003e kubectl run hello-world --image=hello-world:1 --port=8080 --image-pull-policy=never pod/hello-world created The --image-pull-policy=never is telling Kubectl to use the local image and not one from a container registry (Docker, ACR, ECR, GCP)\nTo expose this port for external access (from browser) from outside of the cluster, run:\nPS C:\\\u003e kubectl expose pod hello-world --type=LoadBalancer service \"hello-world\" exposed To confirm your service is running and to get the port number of this exposed service, run:\nPS C:\\\u003e kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-world LoadBalancer 10.111.126.10 \u003cpending\u003e 8080:31589/TCP 45h kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 2d16h You will see the **\\** state of your LoadBalancer if you do not have not a Load Balancer integrated with your cluster. For your local development environment, it is nothing to worry about. You will see that the hello-world service is accessible via port 8080. However, we still don‚Äôt know behind what IPv4 address, this services is available. To get the IPv4 address of your cluster, you type:\nPS C:\\\u003e minikube ip 192.168.75.126 Finally, to access your service, run the cURL command, using the minikube ip address and the TCP port as listed in the kubectl get services output:\nPS C:\\\u003e curl \"http://192.168.75.126:31589\" -UseBasicParsing StatusCode : 200 StatusDescription : OK Content : {72, 101, 108, 108...} RawContent : HTTP/1.1 200 OK Connection: keep-alive Transfer-Encoding: chunked Date: Mon, 06 Apr 2020 13:05:42 GMT Hello World! Headers : {[Connection, keep-alive], [Transfer-Encoding, chunked], [Date, Mon, 06 Apr 2020 13:05:42 GMT]} RawContentLength : 12 You can also use minikube to obtain your service‚Äôs url. To do this, run:\nPS C:\\\u003e minikube service hello-world --url http://192.168.75.126:31589 Useful kubectl commands This first command is important. Some background first‚Ä¶a context is a group of access parameters. Each context contains a Kubernetes cluster, a user, and a namespace. When you are working with multiple contexts off of your development machine, you may run into compatibility issues due to your client version not being compatible with the server API version. All kubectl commands will run against the current context. To check your client version, run:\nPS C:\\\u003e kubectl version --client Client Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.0\", GitCommit:\"9e991415386e4cf155a24b1da15becaa390438d8\", GitTreeState:\"clean\", BuildDate:\"2020-03-25T14:58:59Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"windows/amd64\"} To ascertain your current context, run:\nPS C:\\\u003e kubectl config current-context minikube To list all of your configured contexts, run:\nPS C:\\\u003e kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-desktop docker-desktop docker-desktop docker-for-desktop docker-desktop docker-desktop * minikube minikube minikube The * next to minikube indicates that minikube is your current context.\nIf you are configured to access a cluster hosted from a cloud provider such as Azure, then this context will also be listed.\nTo use a specific context, run:\nPS C:\\\u003e kubectl config use-context docker-for-desktop ","references#References":" Install Kubernetes Install Minikube Kubectl Cheatsheet Minikube‚Äôs built-in Docker daemon "},"title":"Kubernetes on Windows"},"/blog/mentoring/":{"data":{"":"I have written this post to document my experiences of mentoring. I have mentored front-end engineers, back-end engineers and UX designers. I have had the pleasure of helping others as well as learning one of two things about myself along this journey too. If ever you get the opportunity to be a mentor, I recommend you jump at the opportunity. It is a self-rewarding experience.\nSo, what is mentoring?‚Ä¶\nThe definition of Mentoring is the act of advising or training (someone, especially a younger colleague).\nIn her book The Manager‚Äôs Path, Camille Fournier talks about Mentoring. She writes:\n\"The first act of people management for many engineers is often unofficial.\" This has always been the case for me too. I am currently employed as a Principal Engineer, before this, a CTO. In this time, I have neither organised nor carried out an official [backed by a recognised authority] mentoring scheme. It‚Äôs just been something that I do, without fuss but with purpose and pride.\nOddly, I have never been a mentee. If I had then there is a possibility that this in itself may have defined or partially influenced mentoring for me.\nThis is a list of scenarios where I have mentored others in:\nonboarding new company starters, onboarding a new colleague at a similar level as myself onboarding a graduate (their first job since graduating from university) when working on a project together Concerning the above mentioned scenarios, I have both created and coordinated an onboarding programme. This was when I was a CTO. All this was choreographed remotely. Ironically, this is more relevant today than ever. As I write this CV-19 has started to take a grip of the UK and yesterday I heard of the sad news that 2 people had died from it in Southport where I have resided since 2008.\nThis is a bullet list of key ‚Äôthings‚Äô that I have discovered that have helped me through the mentoring process:\ncommunicate what the process of mentoring is to the mentee first, listen, then respond. Don‚Äôt attempt to expedite the process, don‚Äôt forget, it‚Äôs for them, not you! take the time to explain the rationale for a decision take the time to explain why something is not applicable in that particular instance try not to provide answers, but provide strategies (alternatives, is there an easier to do the same, what is the problem we‚Äôre trying to solve) allow for mistakes to be made and always follow them up with a post mortem. We all make mistakes, in some cases, it helps define you. Making a mistake is critical to our development so this is why the next point is important‚Ä¶ ensure you make a safe environment for your mentee to operate in make time but be clear about the amount of time you can give. You will have other responsibilities. Inadvertently, you are forcing the mentee to make decisions. This often encourages the mentee and gives them the confidence to stand on their own two feet. This too is critical for their development work on a real project, albeit, scaled back for safety and to limit the blast radius. It has to be something that matters to the business. This will help the mentee be recognized by their good work. By limiting the hypotheticals, the mentee will then get their hands on a non-fabricated, warts and all, real-life engineering problem to help in the preparation of an important [to them] event - this has meant helping produce the materials for an event as well as assessing and providing feedback develop a personal development plan - used to help keep focus as well as a comparator. This can take up a chunk of time but well worth it plus you‚Äôre holding yourself accountable to the process too! As CTO I led both the architectural and the planned engineering effort that has been key to the strategic direction of that business. Mentoring was an important part of this process and as such, I was always in mentoring mode. To this day, no longer a CTO but still in a senior engineering position, I constantly think about, and act on, ways to help those around me to improve their engineering capabilities (think good engineering principles).\nAlthough not all of my mentoring is official, I do conduct myself in such a way that it benefits those around me. I do this by encouraging my co-workers whenever possible. Here is a list of how I have been able with success, help my co-workers:\nI demonstrate, then I include a co-worker in this process. An example of this is by whiteboarding a problem or solution. I hand over the marker and this leads to them articulating their solution in front of an audience I instigate a technical discussion or articulate an engineering problem. I solicited input from all (introverts and extroverts alike). This encourages my wo-workers to speak up and gain confidence in discussing technical issues in front of an audience I am consistent in the message of working in a safe environment, one where any question can be asked and any view given I define a piece of work‚Äôs guiding principles upfront. This helps in several ways. It defined the focus of the project, what to exclude etc. It also helps shape our collective thinking and finally, it‚Äôs a gentle way into a project instead of a rushing headlong into it without giving it any due diligence Redirect to good engineering principles whenever possible to enforce our foundation of good engineering. I am a Principal Engineers and as such, I have a responsibility to my co-workers and the business to conduct myself in a way befitting a Principal Engineer. Quite simply put, one of the objectives is to help my co-workers in whatever way possible. This can be helping them out on a project. It can be providing feedback on a piece of work or technique. Ultimately, my goals are to be supportive, helpful, insightful, encouraging, guiding, a sounding board and inspirational. All executed respectfully. The people I have worked with and those who I currently work with are important to me. Anything I can do to help, I do. Even if it‚Äôs listening to them sound off. Returning to my goals‚Ä¶I do see some of these being reflected at me but more importantly, I see the product of my mentoring too, which I find extremely satisfying!\nOne of the most humbling times of my life was when I mentored a colleague who, through no fault of his own, was temporarily let go from the company I was a CTO for. We as a company were struggling financially and had to slim down the workforce. It was a sh*t time. It was important to me though from a personal perspective that I didn‚Äôt just sever contact with him. The plan was always to bring him back onboard once things improved. And they did. But during the time that it wasn‚Äôt so great, I would meet-up regularly with him online - he was based in another country. We would discuss many topics; life, technology \u0026 side projects. Where I could, I‚Äôd provide guidance and be a sounding board for him. From time to time I would plan things for him to do. The next time we met up, I‚Äôd review what he had done and provide feedback when necessary. I would like to think that this created a bond between us. Like I say, it was all very humbling as after all, I was still in gameful employment. At some level, it must have been a bitter pill for him to swallow and he never held it against me, which is demonstrative of his good character. We no longer work together but he remains a friend and we do still often catch up online.\nOutcomes from the mentoring process can also be subtle. Just to be clear, it‚Äôs not always explosive or awe-inspiring either. It is what it is and a poor result does not equate to a lack of mentor‚Äôs ability. Generally, poor results are rare. In the one case where I observed poor results, I reported it upwards. The vertical market we were operating in didn‚Äôt float this particular mentee‚Äôs boat. It happens! Also, in my experience, it is always noticeable over time; providing you take a documented snapshot before and after. One source of personal satisfaction is seeing mentees, new and old, interacting with seasoned engineers, observing them standing on their own two feet, adding value to a conversation and project work alike. Best of all, seeing a seasoned engineer asking a mentee for their advice and input on a scenario. That my friends, is extremely satisfying!\nWritten mainly for me, I do hope you‚Äôve found something useful here and who knows, it might even help you with your mentoring journey too."},"title":"How do I mentor?"},"/blog/modern-javascript/":{"data":{"":"This post includes a few notes on ECMA language features that I like as well as some info on memory leaking. I have no doubt that it will read disjointed; I started this eons ago and only now have I decided to publish it.\nA simple reminder of what Node.js is ‚Ä¶it is a set of APIs wrapped around the V8 Engine (written in c++) and is a high-performance JavaScript and WebAssembly engine.","es2015-es6#ES2015 (ES6)":"Class I find Javascript messy at the best of times. When things are messy, personally I find it difficult to see the forest through the trees, and by this I mean, have I coded for all the *-cases (use/edge/corner)? Or worse, can I see the existing defects or bug breaders?! Then there‚Äôs the lack of readability.\nI‚Äôve discussed the use of classes with many Engineers and I have had a mixed reception but in the main, most said they preferred the simplicity of arrow functions. Not sure if there is a right or wrong answer to this (bit like the tabs or spaces‚Ä¶tabs, obvs!)‚Ä¶ and at one time I will have agreed with the majority. Now though is a different story. Like so many others, I too have drank the cool-aid on TypeScript and now the only reason I can see myself opting for Javascript in the future is mainly for legacy reasons.\nComing from an OOP background, I naturally gravitate towards constructs like classes:\nclass Admin extends User { constructor (name) { super(name) this.initialize() } initialize = () =\u003e {} } Destructuring const getProfile = () =\u003e { return {firstname: \"garrard\", lastname: \"kitchen\", married: true, children: 2} } const {firstname, lastname, ...family} = getProfile() console.log(`firstname: ${firstname}`) console.log(`lastname: ${lastname}`) console.log(family) This would result in:\nArrow function Arrow functions are a great addition to the ES spec! Their scope is purely inside of it‚Äôs closure and is not affected by the this context which may hoisted functions fall victum of.\ninitialize = () =\u003e {} ","es2016-es7-language-features#ES2016 (ES7) Language Features":"The Decorator Awesome addition to the EMCA family!\nI‚Äôve used this with great affect with Typescript, and mostly with NestJS solutions.\nThis is a contrived example on how you can use very basic decorator on a class function:\nYou must have configured your solution to use babel\nclass Content { @link('nodejs', \"\u003ca href='https://nodejs.org/en/'\u003eNode.js\u003c/a\u003e\") html() { return `This server language is called nodejs!` } } function link(_find, _replace) { return function(target, key, descriptor) { var old = descriptor.value() descriptor.value = () =\u003e { var n = old.replace(_find, _replace) return n } } } const m = new Content() console.log(m.html()) output:\n[nodemon] restarting due to changes... [nodemon] starting `babel-node index.js` This server language is called \u003ca href='https://nodejs.org/en/'\u003eNode.js\u003c/a\u003e! [nodemon] clean exit - waiting for changes before restart package.json:\n... \"scripts\": { \"start\": \"nodemon --exec babel-node index.js\" }, ... \"devDependencies\": { \"@babel/core\": \"^7.12.3\", \"@babel/node\": \"^7.12.1\", \"nodemon\": \"^2.0.6\" }, \"dependencies\": { \"@babel/plugin-proposal-decorators\": \"^7.12.1\" } ... .babelrc:\n{ \"plugins\": [ [\"@babel/plugin-proposal-decorators\", { \"legacy\": true }] ] } ","es2018-es9#ES2018 (ES9)":"Spread I was reminded of something useful this morning (on the morning I wrote this, originally!) from a youtube video I was watching. JS passes objects (non-primitives) by reference, ergo, memory pointers, so it is possible to effect an object outside of it‚Äôs closure. So, imagine you return an array of objects (e.g. from a service to a controller). It is possible, to effect this array of objects from within the controller. One way I have found to avoid this is by using the spread syntax:\nprivate readonly list: string[] getList() { return list } you can do this:\ngetList() { return [...list] } Obvs, the üëÜ is using an array but you can do this same with an object too {‚Ä¶list}","memory#Memory":"Functions arguments passed by value; always See also spread üëÜ to for advance on how to avoid memory leakage.\nFurther to the above, JS always passes by value (not reference) ALL augments to a function. This means that, if you pass in an argument (primitive or object) into a function, the closure is honoured and therefore any changes made to this value inside the closure is not reflected outside, example:\nlet v: string = \"A\" getValue(v){ v = v + \"B\" } let result = getValue(v) console.log(result) // output: AB console.log(v) // output: A "},"title":"Modern-ish Javascript"},"/blog/my-first-outing-with-dapr/":{"data":{"":"","challenge-1#Challenge #1":"TL;DR: Not as forgiving as I‚Äôd have liked ‚Ä¶\n‚ÑπÔ∏è I was a speaker at a meet-up in Manchester in late 2020. I spoke about Dapr, Keda and the NestJS Framework. My talk topic was on ‚ÄúWriting less code - let your architecture and abstractions help with your *-cases‚Äù. The * in the title is a wildcard for use/edge/corner.\nMy code examples can be found here (includes both docker compose \u0026 Kubernetes manifests) - https://github.com/garrardkitchen/meetup-nov20\nChallenge #1 This took a little longer than I‚Äôd have liked!\nI was using the internal DNS to resolve the port of my redis service. My Redis single instance was deployed via a deployment manifest, along with a LoadBalancer Service - purely to give me remote access.\nI‚Äôd first create a secret, by typing:\n$ kubectl create secret generic db-passwords --from-literal=redis-password='\u003cpassword\u003e' This is the deployment manifest:\napiVersion: v1 kind: Service metadata: name: redis-svc namespace: meetup-dapr-demo labels: run: redis-svc spec: type: LoadBalancer ports: - port: 6379 targetPort: 6379 protocol: TCP selector: run: redis --- apiVersion: apps/v1 # for k8s versions before 1.9.0 use apps/v1beta2 and before 1.8.0 use extensions/v1beta1 kind: Deployment metadata: name: redis namespace: meetup-dapr-demo spec: selector: matchLabels: run: redis replicas: 1 template: metadata: labels: run: redis spec: containers: - name: cache image: redis args: [\"redis-server\", \"--requirepass\", $(PASSWORD) ] env: - name: PASSWORD valueFrom: secretKeyRef: name: db-passwords key: redis-password resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 This deployed correctly.\nI then deployed my Dapr state store component:\napiVersion: dapr.io/v1alpha1 kind: Component metadata: name: mystore namespace: meetup-dapr-demo spec: type: state.redis metadata: - name: redisHost value: redis.meetup-dapr-demo.svc.cluster.local:6379 - name: redisPassword value: \"********\" However, I could not for the life of me give my application access to the state store!\n$ dapr logs -a http-api -k -n meetup-dapr-demo ... time=\"2020-11-06T09:47:02.218770653Z\" level=error msg=\"process component mystore error, redis store: error connecting to redis at redis.meetup-dapr-demo.svc.cluster.local:6379: dial tcp: lookup redis.meetup-dapr-demo.svc.cluster.local on 10.0.0.10:53: no such host\" app_id=http-api instance=http-api-6bc44f8957-q2lvn scope=dapr.runtime type=log ver=0.11.3 Having trying every permutation known to non-gender-specific-person-entity I remembered I was kaing it available behind a service. So, I‚Äôd been using redis.meetup-dapr-demo.svc.cluster.local:6379 when I should have used redis-svc.meetup-dapr-demo.svc.cluster.local:6379.\nOnce I‚Äôd corrected my mistake, it connected without error.\napiVersion: dapr.io/v1alpha1 kind: Component metadata: name: mystore namespace: meetup-dapr-demo spec: type: state.redis metadata: - name: redisHost value: redis-svc.meetup-dapr-demo.svc.cluster.local:6379 - name: redisPassword value: \"********\" ","challenge-2#Challenge #2":"secrets!\nYou‚Äôre application is going to report something similar to this - NOAUTH Authentication required - if you‚Äôre Dapr is deployed to a different namespace to that of your application:\ntime=\"2020-11-06T11:19:06.985273661Z\" level=error msg=\"process component mystore error, redis store: error connecting to redis at redis-svc.meetup-dapr-demo.svc.cluster.local:6379: NOAUTH Authentication required.\" app_id=http-api instance=http-api-7d49cf59d5-9blwf scope=dapr.runtime type=log ver=0.11.3 To circumvent this, you must create a role and binding this to the default ServiceAccount. This role secret-reader allows a get of the secrets resource within the meetup-depr-demo namespace. An example manifest is here:\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: secret-reader namespace: meetup-dapr-demo rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: dapr-secret-reader namespace: meetup-dapr-demo subjects: - kind: ServiceAccount name: default roleRef: kind: Role name: secret-reader apiGroup: rbac.authorization.k8s.io Once deployed, you‚Äôll see something similar to this in your dapr logs:\ntime=\"2020-11-06T11:23:20.529658232Z\" level=info msg=\"component loaded. name: mystore, type: state.redis\" app_id=http-api instance=http-api-7d49cf59d5-kszdz scope=dapr.runtime type=log ver=0.11.3 ‚ÑπÔ∏è This post was created some time ago. Now, we‚Äôre using the Secrets Store CSI Driver to map Azure KeyVault secrets to containers running in our AKS clusters.\nRef: https://docs.microsoft.com/en-us/azure/aks/csi-secrets-store-driver"},"title":"My First Outing With Dapr"},"/blog/nodejs-container-restart-policy/":{"data":{"":"If by accident to deploy a solution using the Node.js Cluster API and do not fork exited processes then the following docker-compose restart_policy will not help you:\ndocker-compose.ymldeploy: restart_policy: condition: on-failure If you‚Äôre using the Cluster API to schedule tasks across your processes, and all forked processes die, then the docker engine will just assume you‚Äôve gracefully shutdown.\nTake this code for example, you will see that it doesn‚Äôt fork another process and therefore, at some point it will no longer process any anything:\ncluster-service.tsimport { Injectable } from '@nestjs/common'; import * as cluster from 'cluster'; import * as os from 'os' @Injectable() export class ClusterService { static clusterize(numCPUs: number, callback: () =\u003e void): void { if (cluster.isMaster ) { const procs = (numCPUs \u003e os.cpus().length) ? os.cpus().length : numCPUs console.log(`GOING TO USE ${procs} PROCESSES`) console.log(`MASTER SERVER (${process.pid}) IS RUNNING `); console.log(`MASTER SERVER (${process.pid}) IS RUNNING `); console.log(`SCHED_NONE: ${cluster.SCHED_NONE}`) console.log(`SCHED_RR: ${cluster.SCHED_RR}`) console.log(`CLUSTER SCHEDULING POLICY: ${cluster.schedulingPolicy}`) for (let i = 0; i \u003c procs; i++) { const worker = cluster.fork(); console.log(`CREATING PROCESS ${worker.process.pid}`); } cluster.on('exit', (worker, code, signal) =\u003e { console.log(`worker ${worker.process.pid} died ${signal || code}`); }); cluster.on('disconnect', (worker) =\u003e { console.log(`worker ${worker.process.pid} disconnected`); }) } else { callback() } } } To mitigate this, you simply fork another process within the exit event handler like this:\nserver.tscluster.on('exit', (worker, code, signal) =\u003e { console.log(`worker ${worker.process.pid} died ${signal || code}, restarting...`); const newWorker = cluster.fork(); console.log(`CREATING PROCESS ${newWorker.process.pid}`); }); To avoid the container not restarting due to lack of process availability to deal with demand in the above scenario, you can‚Äôt use the on-failure condition in the restart_policy. You must use the ‚Äòany‚Äô condition. This section incidentally replaces the old restart sub-option.\ndocker-compose.ymldeploy: replicas: 1 resources: limits: cpus: \"2\" memory: 512M update_config: order: start-first parallelism: 1 restart_policy: condition: any delay: 5s window: 120s placement: constraints: - node.role == worker Caution: You can‚Äôt use max_attempts: 3 in combination with condition: any\nAdditionally, I found one further interesting facts when looking into this issue.\nIf you‚Äôre using Docker Stack Deploy (think stack in Portainer) using a docker-compose file to deploy to your swarm and you‚Äôre using restart: always, then beware, the restart is not supported.\nref: üëÜ compose-file"},"title":"Nodejs Container Restart Policy"},"/blog/nodejs-install-e401/":{"data":{"":"Today I created a simple nodeJS Azure Functions Applicaiton to start building out a PoC and when I tried to install it‚Äôs dependencies like so:\nnpm install I got this little cherub back instead:\nüö´ npm ERR! code E401\nnpm ERR! Incorrect or missing password.\nnpm ERR! If you were trying to login, change your password, create an\nnpm ERR! authentication token or enable two-factor authentication then\nnpm ERR! that means you likely typed your password in incorrectly.\nnpm ERR! Please try again, or recover your password at:\nnpm ERR! https://www.npmjs.com/forgot\nnpm ERR!\nnpm ERR! If you were doing some other operation then your saved credentials are\nnpm ERR! probably out of date. To correct this please try logging in again with: npm ERR! npm login\nnpm ERR! A complete log of this run can be found in:\nnpm ERR! C:\\Users\\garrard.kitchen\\AppData\\Local\\npm-cache_logs\\2022-01-22T16_53_11_848Z-debug-0.log\nOk then, I‚Äôve obviously set my default registry to something other than npmjs!\nThis will not have been an issue or noticeable if I had have been connected to my works VPN as I know for a fact, the upstream sources of our npm Azure DevOps npm feed is in fact https://registry.npmjs.org.\nAnyhow, to work through this slight annoyance, I typed:\nnodejs install --registry https://registry.npmjs.org ... npm install --registry https://registry.npmjs.org added 78 packages, and audited 79 packages in 12s 30 packages are looking for funding run `npm fund` for details found 0 vulnerabilities One npm run prestart and one npm run start later, I was seeing this:\n[17:09:00] Starting compilation in watch mode... Azure Functions Core Tools Core Tools Version: 4.0.3971 Commit hash: d0775d487c93ebd49e9c1166d5c3c01f3c76eaaf (64-bit) Function Runtime Version: 4.0.1.16815 [17:09:01] Found 0 errors. Watching for file changes. Functions: svrless: [GET,POST] http://localhost:7071/api/svrless For detailed output, run func with --verbose flag. info: Microsoft.AspNetCore.Hosting.Diagnostics[1] Request starting HTTP/2 POST http://127.0.0.1:54476/AzureFunctionsRpcMessages.FunctionRpc/EventStream application/grpc - info: Microsoft.AspNetCore.Routing.EndpointMiddleware[0] Executing endpoint 'gRPC - /AzureFunctionsRpcMessages.FunctionRpc/EventStream' [2022-01-22T17:09:02.473Z] Worker process started and initialized. All good, but now I need to change my defaults ‚Ä¶"},"title":"Nodejs Install E401"},"/blog/npm-issues-e401-cert_not_yet_valid/":{"data":{"references#References":"Today a PR Merge resulted in a GHA failure. Sadly, this is not the only CICD pipeline to fail this year! This particular pipeline builds a NodeJS Image, pushes the image to ACR and deploys the service to a production Docker Swarm (on merge to main).\nThis was the error:\nüö´ [3/7] RUN npm install:\n#7 1.469 npm ERR! code E401\n#7 1.470 npm ERR! Unable to authenticate, need: Bearer authorization_uri=https://login.windows.net/736f9f**-09-49-86**-b******31f407, Basic realm=‚Äúhttps://pkgsprodsu3weu.app.pkgs.visualstudio.com/\", TFS-Federated\n#7 1.475\n#7 1.475 npm ERR! A complete log of this run can be found in:\n#7 1.475 npm ERR! /root/.npm/_logs/2021-12-27T09_38_24_060Z-debug.log\nMmmm, E401? ü§î\nI‚Äôve not seen this error before but as it was auth related, assumed the PAT token had expired 1. Google‚Ä¶\nI did not have the actual .npmrc file that was in a GH Secret so I used my own local .npmrc file to confirm I can pull from our organisation‚Äôs npm feed.\ncd \u003cproject-root\u003e rm node-modules npm cache clean --force npm install It worked. So, I had my first fallback option - regenerate PAT, register centrally for a reminder of when the PAT is to expire and update GH secret. However, I was not yet done. I had not yet reproduced verbatim the pipeline, egro a docker build.\nSo, I ran this:\ndocker build -t \u003cimage:tag\u003e . Oh no! The error has returned!\nThis is the Dockerfile:\nFROM node:14.4.0 WORKDIR / COPY . . RUN npm install RUN echo 'module.exports = ' | cat - node_modules/@\u003credacted\u003e/\u003credacted\u003e/dist/libs/\u003credacted\u003e-lib/index.js \u003e temp \u0026\u0026 mv temp helpers/index.js RUN cd helpers \u0026\u0026 ls -la RUN head -10 helpers/index.js ENV PORT=80 EXPOSE 80 CMD [\"npm\", \"run\", \"\u003credacted\u003e\"] Apart from it not being the LTS, there was nothing obviously wrong with it plus it had been building ok leading up to this.\nThis a the credentials part of the .npmrc:\nregistry=https://pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/registry always-auth=true //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/registry/:username=\u003cany-value-not-empty\u003e //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/registry/:_password=\u003cBase64-encoded-PAT\u003e //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/registry/:email=\u003cemail-is-not-used\u003e //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/registry/:always-auth=true According to this Microsoft documentation post ‚û° npm scopes, the token structure was incomplete. I corrected the structure:\nregistry=https://pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/registry always-auth=true //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/registry/:username=\u003cany-value-not-empty\u003e //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/registry/:_password=\u003cBase64-encoded-PAT\u003e //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/registry/:email=\u003cemail-is-not-used\u003e //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/registry/:always-auth=true //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/:username=\u003cany-value-not-empty\u003e //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/:_password=\u003cBase64-encoded-PAT\u003e //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/:email=\u003cemail-is-not-used\u003e //pkgs.dev.azure.com/\u003corg-name\u003e/_packaging/\u003cfeed-name\u003e/npm/:always-auth=true I re-ran the docker build -t \u003cimage:tag\u003e . This time I got a different error but the original E401 error had gone away!\nThe new error:\nüö´ #7 43.61 npm ERR! code CERT_NOT_YET_VALID\n#7 43.61 npm ERR! errno CERT_NOT_YET_VALID\n#7 43.61 npm ERR! request to https://xuavsblobprodsu6weus12.blob.core.windows.net/b-7a3f75bdbbf3432bbe2621e93c98932a/86CDB768B8C395B149741963D831555B55E3458************.blob?sv=2019-07-07\u0026sr=b\u0026si=1\u0026sig=LUCuO42mrmOAx5Nzds9RWS0v2qL%2FwbB86c%3D\u0026spr=https\u0026se=2022-01-12T11%3A42%3A24Z\u0026rscl=x-e2eid-59793e65--a727df6e-***********-session-59793e65-*******-a727df6e-8c628cc8\u0026rscd=attachment%3B%20filename%3D%22string-width-4.2.3.tgz%22 failed, reason: certificate is not yet valid\nMmmm, CERT_NOT_YET_VALID. I‚Äôd not seen this error before. Google‚Ä¶\nAfter a short period of online research üëÄ I found a suggestion about being explicit in setting the default npm registry. This meant I had to insert RUN npm config set registry http://registry.npmjs.org before the RUN npm install command. The resulting Dockerfile then looked like this:\nFROM node:14.4.0 WORKDIR / COPY . . RUN npm config set registry http://registry.npmjs.org RUN npm install RUN echo 'module.exports = ' | cat - node_modules/@\u003credacted\u003e/\u003credacted\u003e/dist/libs/\u003credacted\u003e-lib/index.js \u003e temp \u0026\u0026 mv temp helpers/index.js RUN cd helpers \u0026\u0026 ls -la RUN head -10 helpers/index.js ENV PORT=80 EXPOSE 80 CMD [\"npm\", \"run\", \"prod\"] I re-ran the docker build -t \u003cimage:tag\u003e . and success! ü•≥\nI updated the appropriate GH Secret with the modified .npmrc file and asked the author of the PR that had reported this issue originally to make the 1 line change to the Dockerfile. He made the change, prompted for another PR review and merged to main on approval. The GHA ran successfully! The usual monitoring post deploy and feature/fix was confirmed and I set about updating internal documentation providing instructions on what to do to remediate for others and notifying all via slack that of this.\nReferences Npm scopes 1 - secrets that expire need to be registered centrally on a system that can notify you in advance, giving you ample time to remediate. For our Azure AAD SP (Service Principals) client secrets, we run an Automation Runbook each day that traverses the Azure Resource Graph and alerts me via email of those SP that will be expiring within 30 days."},"title":"Npm E401 and CERT_NOT_YET_VALID"},"/blog/permission-denied-while-trying-to-connect-to-the-docker-daemon-socket/":{"data":{"":"Out of the blue today, my first day back after Christmas break, I got this when running a GH Actions Workflow on one of our Self-Hosted Linux Runners üò±:\nüö´ Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get ‚Äúhttp://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json\": dial unix /var/run/docker.sock: connect: permission denied ‚ÑπÔ∏è We have several GitHub Self-Hosted Runners running on Linux and Windows O/S that produce, amongst other artefacts, Linux and Windows images. These images are pushed to ACR. We‚Äôre in the process of migrating our on-premise real-estate - IIS Web apps, Windows Services, Docker swarm containers - to AKS as well as migrating our SQL Server AG to Azure. We‚Äôre using Self-Hosted Runners as we have spare compute capacity and some of our applications have a dependency on a legacy NuGet server which requires our CI pipelines to run in our network. We are in the process of also migrating these legacy packages to our Azure DevOps NuGet Feed as part of our modernization initiative. This modernization initiative encompasses upgrading our runtimes to .NET Framework 4.8 and .NET 6.0. It had been running fine prior to my break so what gives? I started to investigate‚Ä¶\nI logged in to the Linux VM where this particular Self-Hosted Runner is hosted with the same credentials as used when I installed the Self-Hosted Runner originally. I used the following command to confirm the same outcome:\ndocker ps Yup, same thing.\nThe next configuration I wanted to check was whether this user is a member of the docker group so I used this command:\nsudo groups \u003cuser\u003e Mmmmm, that‚Äôs odd. This user wasn‚Äôt a member and therefore begs the question, how did this ever work in the first place?!!\nI added this user using this command:\nsudo usermod -a -G docker \u003cuser\u003e I ran docker ps again but still no dice. ü§î.\nI then checked the status of the docker service using this command:\nsudo systemctl status docker It reported:\n‚ÑπÔ∏è Active: active (running) since Thu 2021-09-16 14:13:04 UTC; 3 months 20 days ago Ok, what next? ü§î\nI decided to restart the self-hosted service so I entered these commands:\ncd actions-runner sudo ./svc.sh start This is when I saw these failures:\nüö´ Dec 19 22:01:15 redacted runsvc.sh[291703]: 2021-12-19 22:01:15Z: Runner connect error: The HTTP request timed out after 00:01:00.. Retrying unt‚Ä¶econnected.\nDec 19 22:02:35 redacted runsvc.sh[291703]: 2021-12-19 22:02:35Z: Runner reconnected.\nJan 06 14:42:21 redacted runsvc.sh[291703]: 2022-01-06 14:42:21Z: Running job: deploy\nJan 06 14:42:41 redacted runsvc.sh[291703]: 2022-01-06 14:42:41Z: Job deploy completed with result: Failed\nJan 06 14:46:19 redacted runsvc.sh[291703]: 2022-01-06 14:46:19Z: Running job: deploy\n‚Ä¶\nI restarted the Self-Hosted Runner using these commands:\nsudo ./svc.sh stop sudo ./svc.sh start Then I logged out and back in again to confirm docker access docker ps and finished off by re-running the failed GH Action Workflow. ü•≥ Equilibrium is once again restored. As per protocol, I shared issue and resolution with our IT Team in case this crops up again when I‚Äôm not online to help."},"title":"Permission Denied While Trying to Connect to the Docker Daemon Socket"},"/blog/principles/":{"data":{"":"","#":"I have written this post as a method to document what I see as the basics, foundations if you will, for good engineering. Undoubtedly if you are a seasoned engineer, you will recognised all of these principles, less so, if you‚Äôre just starting out.\nMost Engineers are fully versed in the foundations of writing quality, efficient, succinct and testable code. As a Principal Engineer, one of my responsibilities is to ensure that these (1) foundations are recognised by the engineers and (2) are adhered to by all engineers.\nHere‚Äôs a list of concepts that for me, constitute good engineering principles:\nThese are in alphabetical order and not in order of importance\nClean and readable code Code reviews Coding standards Composition over inheritance Defensive coding Do no more DRY KISS Occam‚Äôs razor Premature optimization Refactor Separation of Concerns SOLID Testing YAGNI Other sections:\nMy pattern discovery Being a Principal Engineer Discussion point References Clean and readable code Clean and readable code is always better than clever code (ask any engineer who has to extend or maintain a clever piece of code!)\nI‚Äôve seen a lot of code recently that should never have got to the shape it has. Complicated code requires time to understand, then time to add functionality. Complicated code also happens to more difficult to recall so each time you need to go near it, you have to relearn it and added to this, any changes made to improve it, most likely have not been applied in full so they‚Äôll be a right old mixture of good, bad and the ugly thrown into the mix.\nA good measure of how bad a codebases is, and I‚Äôm going to plagiarise somebody else‚Äôs analogy here, is by stepping through an interactive debug session. If you get momentarily distracted by a fly, then immediately return to the debugging and you do not know where the feck you are in the execution of the code flow, then it‚Äôs a bad codebase!\nIt‚Äôs the responsibility of a Tech Lead or architecture to stop code bases ending up this way.\nCode reviews It should only contain helpful and constructive comments and/or implementation questions. This process is not there to caress egos (that‚Äôs for your mother to do!!). One useful by-product of code reviews is conveying of your team‚Äôs exacting coding standard and attention to deal, to new starters. So, the quicker the new starter pushes a commit, the better!\nCoding standards (provide a template of core standards then stand back and let the team thrash out the rest - wear protection!)\nAlthough important, it‚Äôs not the end of the world if some of the granular details differ between teams. The important thing here, in my opinion, is that each team know where to find their cheese. Most engineers in a team have a common set of standards they adhere too. The big things like solution structure, naming conventions, testing (AAA, GWT), pluralization, documentation structure (including README) all need to be consistent.\nComposition over inheritance (avoid class tree exploitation! - think Strategy pattern - GoF)\nThe above-bracketed statement says it all! Inheritance tends to back you into a corner especially when you consider the OCP.\nDefensive coding (guard against invalid class method parameters and accidental null assignment to class properties instead of an equality condition!)\nThis is one example of defensive coding:\nclass User(string firstnaame, string lastname, int age) { if (null == firstname) { throw new NullReferenceException(\"Firstname cannot be null\") } ... The above demonstrates an example of defensive coding. The first is that we need to test for valid constructor parameter values when instantiating a class.\nThe second, is to avoid mistakes that might not be picked up by your compiler. For instance, a common mistake doing this:\nif (firstname = null) A .NET Compiler is more than happy allowing this above syntax, as, after all, it‚Äôs an assignment operator and not a equality operator as in above in the class constructor. By switching these around, you‚Äôre making a positive pattern changing and should avoid making this silly mistake again.\nDo no more (and do no less - thank you eXtreme Programming!).\nIf you code outside the scope, you‚Äôre in danger of creating code that isn‚Äôt used or needed. The worse thing about this is that others will have to maintain this code. How can this be? Well, it‚Äôs common - think HTTP chaining - for code not to be culled especially if there is a disconnect between these dependencies and there‚Äôs no IDE/compiler to shout at you.\nDRY (don‚Äôt repeat yourself)\nCode analysis tools help here, but you‚Äôre not always going to have access to these tools.\nOne way to help identify code that does the same thing is by refactoring. If you keep your code method frame small (~20 lines), and you have a good naming standard for methods (e.g. noun+verb with accurate alighment to business capability - think DDD), have unit tests with a high code coverage percentage, then this should be all you need to help you avoid writing duplicate code.\nKISS (keep it simple, silly)\nThis to a certain extent, goes hand in hand with avoiding premature optimization. We all like the big picture yes? This doesn‚Äôt mean we need to do deliver on this it right now! You just need to know the boundaries of this piece, which, if greenfield, then you won‚Äôt have any metrics to tell you the actual demand. Think Capacity planning; what this piece of work needs to do based on current expectations. For example\nDo we need multiple servers? Yes, I think Why do we need multiple servers? Mmmmm, because I read it somewhere Do you have the metrics that support your argument for multiple servers? Wait, what? Next! A colleague recently shared with me the architecture of their side project. They are using AWS and I have 2 certifications in AWS (Developer and Solutions Architect). I quickly went into HA/scaling/resilience/durability/DR overdrive, following it up with a verbal dump on what tech they should use. This was all wrong. They did not know their service demand. Following my initial advice, will have increased their cost; unnecessarily. I did, you‚Äôll be glad to hear, re-affirm their decision (may have made 1 or 2 helpful suggestions) shortly after [~2 hours].\nYeah, think big but don‚Äôt deliver big without a customer base; as this, in my experience, will result in a huge waste of time, effort and money. Plus, sometimes, you don‚Äôt really know where something is going to take you, and my advice here is to roll with it. This last piece of advice is particularly pertinent if you‚Äôre starting up.\nOccam‚Äôs Razor This is a problem-solving principle.\nThe definition of this is: ‚ÄúEntities should not be multiplied without necessity‚Äù. It is sometimes paraphrased by a statement like ‚Äúthe simplest solution is most likely the right one.\nOccam‚Äôs razor says that when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions. Good advice\nSuppose there are two competing theories on why something is not working. Normally, the case that requires the least amount of assumptions is correct. So, the more assumptions you have to make means it more likely to be more unlikely.\nPremature optimization Avoid premature optimization and all conversations relating to optimization until you know the facts. This will be futile until you‚Äôve metrics to better inform you.\nI‚Äôve hit this numerous times when planning for microservices and bounded contexts, in particular, on green-field projects. What should we include and where? Should we separate claims away from users for instance? Will the demand for Claims be greater than for users? Who knows?! You don‚Äôt until you have some metrics behind you. You can always merge or break them [microservices] up later.\nAnother area that I believe this encompasses is splitting code up across multiple files and folders. If it‚Äôs a PoC, a sample piece of code, or something that has a short shelf life, just keep it in one file. When it‚Äôs the right time - moving out of PoC/other - then you can consider optimizing it. Up until then, it‚Äôs a huge waste of time and effort.\nArchitecture is a great example of when not to prematurely optimize. Architecture normally infers cost. Generally, the more of something, the greater the cost. This could mean for a startup the difference between survival and their demise. Adopting a guiding principle of being frugal from the outset, is a prudent and wise decision. What this means is that you‚Äôre always looking for the most cost-effective way of accomplishing your goal. So, if you don‚Äôt know your demand, it means you opt for a single server instead of having a HA cluster of 3 master nodes and 5 worker nodes! Down from 8 servers to 1 which on a month by month basis during development and beta/early releases could mean the saving of thousands of pounds sterling.\nSadly, I‚Äôve come across a few startup that have failed just because they ran out of cash early on. It‚Äôs a real shame for all involved.\nRefactor ‚Ä¶refactor refactor\nDon‚Äôt save this until the end of a piece of work ‚Ä¶ you‚Äôre bound to miss something and possibly add to your team‚Äôs tech debt. Plus, if you push your commits to a PR, you‚Äôll get your ass handed to you by your peers!\nThings to consider here are DRY and TDD. Both will nudge you towards a proper refactoring effort.\nSeparation of Concerns (think MVC, CQRS, bounded context, etc‚Ä¶)\nIt‚Äôs all about doing the right this in the right place! I recently ran, architected and co-developed a project that involved our own hosted solution, a solution hosted on Azure and a solution hosted on the Twilio Cloud (Twilio Serverless Functions). Originally, the requirements did not include the Twilio Cloud and would have required a bucket load more effort if we‚Äôd stuck with that brief. Thankfully, I chose to take full advantage of what Twilio has to offer and used a combination of Twilio Flow and Twilio Serverless Functions. By establishing these SoCs it meant:\na less stressful implementation a light touch to our own hosted solutions a satisfying amount of fun working with Serverless (has been my favourite and advocated approach for several years!) a time saving it revealed a range of options when dealing with specific edge and corner cases which, again, giving us a further time savings. SOLID These are the SOLID principles:\nSingle Responsibility Principle Open Closed Principle Liskov Principle Interface Segregation Principle Dependency Inversion Principle Single Responsibility Principle A class (no method) should have one and only one reason to change, meaning that a class (or method) should have only one job.\n‚ÄúWhen a class has more than responsibility, there are also more reasons to change that class‚Äù\nHere‚Äôs an example of a class )purposefully awful for illustrative purposes):\nclass User() { public string Username {get; set;} public string Fullname {get; set;} private readonly ILogger _logger; private IDbContext _db; public User() { _logger = new Logger() _db = new UserContext(); } public Task\u003cUser\u003e GetProfile(string username) { ... _logger.Info($\"Found profie for {username}\") return this; } } You could say that the above includes both a model responsibility and a service responsibility. These should be split into two separate .NET types, as in this example:\nclass User() { public string Username {get; set;} public string Fullname {get; set;} public User(string username, string Fullname) { ... } } class UserService() { private readonly ILogger _logger; public UserService(ILogger _logger, IDbContext db) { _logger = _logger _db = db; } public Task\u003cUser\u003e GetProfile(string username) { ... _logger.Info($\"Found profie for {username}\") return user; } } Here are the benefits of principles:\nReduces complexity in your code Increases readability, extensibility, and maintenance of your code Reusability and bug breading Easier to test Reduces coupling by removing dependency between methods Open Closed Principle Objects or entities should be open for extension, but closed for modification. So, what does this mean? Let‚Äôs break this down to two statements:\nOpen for extension Closed for modification Open for extension: This means that we need to design our classes in such a way that it‚Äôs new responsibilities or functionalities should be added easily when new requirements come.\nOne technique for implementing new functionality is by creating new derived classes. A derived class will inherit from base class. Another approach is to allow the ‚Äòclient‚Äô to access the original class with an abstract interface. I sometimes think of this simply as removing if statements by extension but I‚Äôm not convinced everybody would agree with this assessment though.\nSo, in short, if there‚Äôs an amendment or any new features required, instead of touching the existing functionality, it is better to create new derived class and leave the original class implementation. Well, that‚Äôs the advice! I worry about the class explosion and if you‚Äôre attempting to do this on top of not so perfect code!\nClosed modification: This is very easy to explain‚Ä¶only make modifications to code if there‚Äôs a bug.\nThis sample looks at delegating method logic to derived classes.\npublic class Order { public double GetOrderDiscount(double price, ProductType productType) { double newPrice = 0; if (productType == ProductType.Food) { newPrice = price - 0.1; } else if (productType == ProductType.Hardware) { newPrice = price - 0.5; } return newPrice; } } public enum ProductType { Food, Hardward } Can rewrite, still using base implementation (think decorator pattern):\npublic class Order { public virtual double GetOrderDiscount(double price) { return price; } } public class FoodOrder : Order { public override double GetOrderDiscount(double price) { return base.GetOrderDiscount(price) - 0.1; } } public class HardwareOrder : Order { public override double GetOrderDiscount(double price) { return base.GetOrderDiscount(price) - 0.5; } } Liskov Principle Definition: ‚ÄúLet q(x) be a property provable about objects of x of type T. Then q(y) should be provable for objects y of type S where S is a subtype of T.‚Äù ‚Ä¶ clear as mud right?\nAll this is stating is that every subclass/derived class should be substitutable for their base/parent class.\nThe example below demonstrates a violation of the Liskov principle, as by replacing the parent class (SumEvenNumbersOnly-\u003eCalculator), this does compromise the integrity of the derived class as the higher-order class is not replaced by the derived class. Here, both cal and eventsOnly variables will be the same:\n... var nums = new int[] {1, 2, 3, 4, 5, 6, 7}; Calculator cal = new Calculator(nums); Calculator evensOnly = new SumEvenNumbersOnly(nums); ... public class Calculator { protected readonly int[] _numbers; public Calculator(int[] numbers) { _numbers = numbers; } public int Sum() =\u003e _numbers.Sum(); } public class SumEvenNumbersOnly : Calculator { public SumEvenNumbersOnly(int[] numbers) : base(numbers) { } public new int Sum() =\u003e _numbers.Where(x=\u003ex % 2 == 0).Sum(); } Here we have changed the assumed base class to an abstract class. Now, it can‚Äôt be instantiated and instead, must be inherited. This ensures the derived classes must implement the method detail. So, even if we replace the type declaration with the higher-order class, we should still get the intended result:\n... var nums = new int[] {1, 2, 3, 4, 5, 6, 7}; Calculator cal = new SumAllNumbersOnly(nums); Calculator evensOnly = new SumEvenNumbersOnly(nums); ... public abstract class Calculator { protected IEnumerable\u003cint\u003e _num; protected Calculator(IEnumerable\u003cint\u003e num) { _num = num; } public abstract int Sum(); } public class SumAllNumbersOnly : Calculator { public SumAllNumbersOnly(IEnumerable\u003cint\u003e num) : base(num) { } public override int Sum() =\u003e _num.Sum(); } public class SumEvenNumbersOnly : Calculator { public SumEvenNumbersOnly(IEnumerable\u003cint\u003e num) : base(num) { } public override int Sum() =\u003e _num.Where(x =\u003e x % 2 == 0).Sum(); } Interface Segregation Principle A client should never be forced to implement an interface that it doesn‚Äôt use or clients shouldn‚Äôt be forced to depend on methods they do not use.\nTake the following interface:\npublic interface IAllTheThings { Task\u003cIAsyncEnumerable\u003cClaim\u003e\u003e GetClaims(string username); Task\u003cIAsyncEnumerable\u003cUser\u003e\u003e GetUsers(string team); Task\u003cUser\u003e AddUsers(User user); } There‚Äôs a clear distinction in responsibilities that are being suggested here by the contract name. Sufficed to say, these should be split across different interfaces:\npublic interface IUser { Task\u003cIAsyncEnumerable\u003cUser\u003e\u003e GetUsers(string team); Task\u003cUser\u003e AddUsers(User user); } public interface IClaim { Task\u003cIAsyncEnumerable\u003cClaim\u003e\u003e GetClaims(string username); } Dependency Inversion Principle There are 2 rules here:\nHigh-level modules should not depend on lower-level modules. Both should depend on abstractions. Abstractions should not depend upon details. Details should depend upon abstractions. Let‚Äôs deal with the first rule first. High-level means policy, business logic and the bigger picture. Lower-level means, closer to the bare metal (think I/O, networking, Db, storage, UI, etc‚Ä¶). Lower-level tend to change more frequently too.\nThese two examples show perfectly the before and after of the move to a ‚Äòdepend on abstraction‚Äô:\npublic class BusinessRule { private DbContext _context; public BusinessRule() { _context = new DbContext(); } public Rule GetRule(string ruleName) { _context.GetRuleByName(ruleName); } } public class DbContext { public DbContext() { } public Rule GetRuleByName(string name) { return new Rule(new {Name = \"Allow All The Things\", Allow = false}) } } After changing to an abstraction:\npublic interface IDbContext { Rule GetRuleByName(string name); } public class BusinessRule { private IDbContext _context; public BusinessRule(IDbContext context) { _context = context; } public Rule GetRule(string ruleName) { _context.GetRuleByName(ruleName); } } public class DbContext : IDbContext { public DbContext() { } public Rule GetRuleByName(string name) { return new Rule(new {Name = \"Allow All The Things\", Allow = false}) } } With the above change, the DbContext can be any class as long as it inherits from the IDbContext interface and has a method with a signature of Rule GetRuleByName(string name).\nThe above is demonstrative of the 2nd rule; do not depend on the detail. As you can see, in the example above, we‚Äôre depending on an interface method contract and the actual implementational detail is being dealt with by the Lower-level class.\nThe above example includes Dependency Injection. Although you can accomplish IoC with DI, they are not the same thing. IoC does not mention anything about the direction of the dependency.\nGeneralization restrictions The presence of interfaces to accomplish the Dependency Inversion Pattern (DIP) has other design implications in an object-oriented program:\nAll member variables in a class must be interfaces or abstracts All concrete class packages must connect only through interface or abstract class packages No class should derive from a concrete class No method should override an implemented method All variable instantiation requires the implementation of a creational pattern such as the factory method or the factory pattern, or the use of a dependency-injection framework. Testing (unit/functional, including concepts like TDD \u0026 BDD and frameworks)\nFor testing to be a success, the details are key. These details will come in the form of a specification or from a verbal conversation (always to be confirm in writing later). If you‚Äôre lucky, these test cases will be included as ACs (Acceptance Criteria) in the Scrum Story Description.\nTaking a test driven development approach to writing code often results in:\na reduction in verbose code less post-deployment bug fixing succinct (do no more, no less than is required), structure and logic. Testing is important. Obviously! I often refer to testing as ‚Äòhaving your back‚Äô. It ensures you don‚Äôt break existing functionality when implementing new functionality or dealing with tech debt. It also protects new engineers from breaking things as well as extant engineers who may have touched this repository many times before.\nTests aren‚Äôt just for new functionality either. If you change extant functionality or class responsibilities you must modify extant tests or create new tests. Ideally, your CI build pipeline should run tests every time time a commit(s) is pushed to a PR or Draft PR. This last step is here, to again, have your back and to safeguard against erroneous code poluting your codebases and getting into production.\nIn the .NET world, there are many testing frameworks available; xUnit, NUnit, MSTest to name a few. There are also many mocking frameworks available; Moq, NSubstitute, JustMock, again, to name a few. Frameworks like these help make the testing process and overall experience less painful and cumbersome and some might even say it makes this part of development, pleasurable!\nMy .NET Core testing and mocking preferences are xUnit \u0026 Moq and my javascript (including node.js) testing framework preference is Jest.\nThis code sample shows how both a testing and mocking frameworks compliment each other:\nusing Moq; using Xunit; namespace BasicAAATestExample { public interface IUser { string GetFullname(); string Firstname { get; set; } string Lastname { get; set; } } public class User : IUser { public string Firstname { get; set; } public string Lastname { get; set; } public string GetFullname() { return $\"{Firstname} {Lastname}\"; } } public class Notify { private IUser _user; public Notify(IUser user) =\u003e _user = user; public string GetMessage() =\u003e $\"{_user.GetFullname()} has been notified\"; } public class NotifyTests { [Theory] [InlineData(\"Garrard\", \"Kitchen\", \"Garrard Kitchen has been notified\")] [InlineData(\"Charles\", \"Kitchen\", \"Charles Kitchen has been notified\")] public void GivenGetMessageIsCalled_WhenFirstAndLastNameExist_ThenReturnsANotificationMessage(string firstname, string lastname, string expected) { // arrange var mockUser = new Mock\u003cIUser\u003e(); mockUser.Setup(x =\u003e x.GetFullname()).Returns($\"{firstname} {lastname}\"); var sut = new Notify(mockUser.Object); // act string message = sut.GetMessage(); // assert Assert.Equal(expected, message); mockUser.Verify(x =\u003e x.GetFullname(), Times.Once); } } } The single unit test above follows the AAA (Arrange, Act, Assert) pattern and is a common way of writing unit tests for a method under test:\nthe Arrange section of a unit test method initializes objects and sets the value of the data that is passed to the method under test the Act section invokes the method under test with the arranged parameters the Assert section verifies that the action of the method under test behaves as expected. There are a few standards I adhere to when it comes to writing tests. In the sample unit test above these standards include:\nthe method name (GWT) the comment blocks of arrange, act and assert the name of the mock instantiated object (mock\u003cClass\u003e) the class name of the SUT - system under test - (sut). YAGNI (you ain‚Äôt going to need it)\nDo no more, and no less than is required. You do not want to have to maintain code that is never used or produce code that others have to maintain unwittingly. It‚Äôs very difficult to future proof your code if you do not know what‚Äôs going to happen, let alone without a specification! It‚Äôs a guess at best so don‚Äôt waste your time or others. Keeps things concise, succinct and simple.\nMy pattern discovery I‚Äôm a huge fan of patterns, especially cloud architectural patterns but sometimes, they add unnecessary complicity so beware!\nWhen I first started learning about patterns - some 18 years ago - I went through a few codebases I was involved with at the time to see if I‚Äôd subconsciously been repeatedly using a pattern ‚Ä¶ and I had! It was the lazy loading pattern‚Ä¶which I continue to use regularly today!\nBeing a Principal Engineer As a Principal Engineer, I consider the above as the foundation for writing quality code. The objective of this list, in conjunction with the message I propagate via this list, during discussions, evidence from my own work and by leading from the front within my role, is one of a reminder to me and my colleagues of best practice and commitment to quality and good practice. As with all foundations, it forms the base from which more advanced concepts or approaches can be learned. An important part of this practice is heuristic - enabling a person to discover or learn something by themselves. So, how do I go about doing this?\nThese are some of the activities I execute to embed good engineering principles:\n1-2-1 Group conversations Advocate online learning platforms such as Pluralsight or Udemy. For the more keen Engineer, I also recommend certification. YouTube is another favourite of mine. With YouTube, you can tag recordings, therefore building up a catalogue of materials that you can make public Workshops Brown bags Capture How To Do‚Äôs in wikis or similar Coding advice/tips (e.g. when to use Task instead of an Async method) Take the time to explain questions about implementation reasons in DVCS Pull Requests Share blog posts \u0026 other materials across multiple channels Compile a learning profile for an individual The coding advice/tips above are interesting ones. As professionals, we always want to improve our ability to code, how we approach problems, etc‚Ä¶, and in doing so we want our colleagues to benefit from our experience. I recently became reacquainted with coding katas. As a black belt in Ju-Jitsu I am well versed in what a kata is. Katas can also be used to remind, stretch and improve our core coding capability. The last time I used a kata in programming was 10+ years ago. This was when I was first introduced to TDD. A favourite development book of mine is ‚ÄòThe Art of Unit Testing‚Äô by Roy Osherove. It is the first edition. For many years I had it as a click-thru purchase option on a previous blog site of mine. I‚Äôve not really participated in many katas since. I have written a few though and now having been reintroduced to them and reminded of their potential, as a Principal Engineer I can see it as an invaluable tool. One thought I‚Äôve had is to use it as a framework to assess an Engineer‚Äôs current capability and then use during pair programming to help share coding techniques, approaches and standards.\nPair programming is a wonderful technique for propagating developer skills (think how to use cloud services), approaches to coding (think TDD and problem solving), embed team coding standards and code review in realtime. Pair Programming is an Extreme Programming technique. It is not a mentoring or coaching technique but some do use it for this. Quite often, I find I only participate in pair programming is one of two use cases. (1) if the subject I‚Äôm investigating is new (important to have shared knowledge) and (2) when I‚Äôm helping an Engineer to overcome an esoteric issue. You know what they say? ‚Ä¶a problem shared is a problem halved! However, now, I‚Äôll be including Pair Programming in conjunction with katas as part of my techniques to stretch the Engineer‚Äôs muscle memory (including mine!).\nI love hacking away at code as much as the next Engineer. Hacking code is a great way to experiment and to get out of the starting gate. However, when it comes to pair programming I do like to give it the necessary due diligence. By this I am referring to allowing for a few moments up front to deliberate and agree on what‚Äôs required. This checklist guides me and ensures up front I set off in the right frame of mind:\nthe objective of the pair programming exercise (think the desired outcome - you might even want to frame this with a Story Description; AS A, I WANT, SO THAT) what libraries (3rd party) will we need (think cloud provider SDKs and vendor client APIs) how are we going to test the code we write (think unit tests, integration, functional [e2e] as well as your approach e.g. TDD). After the session has finished I like to perform one final task. This is to document findings/learnings and areas that require further investigation. This is normally helped by capturing notes as we go along.\nAs a side note to TDD, with modern compilers (think Roslyn in the .NET world) and even linting to a certain extent, you know if something will fail - if a reference type (.NET) does not exist yet - as your IDE will be screaming at you, so I don‚Äôt run tests that are missing these reference types (think classes and interfaces in the .NET world).\nDiscussion point I‚Äôm sure I‚Äôm not alone here when I say, having the time available for 2 Engineers to code together for skills transfer etc is a challenging one. An agile sprint doesn‚Äôt facilitate this. This is something that I often refer to as having the ‚Äòspace to learn‚Äô. The pressures of a sprint often, sadly, works against this. This is doubly as difficult, if your sprint is made up of technical debt, BAU, Ad-hoc etc‚Ä¶ Timeboxing ‚Äôeffort‚Äô into percentages doesn‚Äôt always present an obvious education path for your Engineers either. Having a day (developer day or similar) dedicated to learning also never really quite works out the way it‚Äôs meant too, plus, ‚Äòa day‚Äô?! In my experience, this, and trying to cram genius into a time box also never quite works either. After all, you can‚Äôt schedule genius, in the same way, you can‚Äôt guarantee that the best Engineers are in your locality, or that the best time for Engineers to work is between 9-5.\nWhat is the answer? A mixture of all the above, at hock and at scheduled times, to ensure quality and advancement of skills.\nWhen I do speak out regarding the above, I inevitably also lead this conversation into Engineering not having the kit [hardware \u0026 software] they need. Engineers require the software and hardware they deem as necessary to be effective in their role. I once gave an analogy of, not giving Engineers the right kit is like giving a roller brush and a Pogo stick to Michelangelo to paint the Sistine Chapel ceiling. He‚Äôll manage it ‚Ä¶ eventually, but the attention to detail and accuracy will be woefully inadequate.\nWritten mainly for me, I do hope you‚Äôve found something useful here, and who knows, it may even help you with your engineering journey too.\nReferences The pragmatic programmer\nYAGNI\nXP\nDependency inversion principle\nOOP design patterns\nInversion of Control Containers and the Dependency Injection pattern\nWrite your tests"},"title":"Good Engineering - Principles"},"/blog/prompts/":{"data":{"":"","automatically-including-prompts-in-copilot-chat#Automatically Including Prompts in Copilot Chat":"To have Copilot Chat automatically use your custom prompts, configure the github.copilot.chat.codeGeneration.instructions setting in your workspace settings (e.g., .vscode/settings.json):\n{ \"github.copilot.chat.codeGeneration.instructions\": [ { \"file\": \"readme.prompt.md\" } ] } This tells Copilot Chat to include the content of readme.prompt.md as context for your requests.","creating-custom-prompts#Creating Custom Prompts":"You can create reusable prompts by adding .prompt.md files in your repository‚Äôs .github/prompts directory. Each file should contain a specific instruction or context you want Copilot to use.\nSteps:\nCreate a .github/prompts directory in your repository if it doesn‚Äôt exist. Add a Markdown file (e.g., readme.prompt.md) with your prompt content. Example (.github/prompts/readme.prompt.md):\nAlways use 2 spaces for indentation and single quotes for strings. ","example#Example":"Suppose you want Copilot to always follow your project‚Äôs code style. Create .github/prompts/codestyle.prompt.md:\nAlways use 2 spaces for indentation and single quotes for strings. Then, update .vscode/settings.json:\n{ \"github.copilot.chat.codeGeneration.instructions\": [ { \"file\": \"codestyle.prompt.md\" } ] } Now, Copilot Chat will automatically consider your code style instructions.","references#References":" GitHub Copilot Chat Cookbook GitHub Copilot Chat Documentation Configuring Copilot Chat Best Practices for Using GitHub Copilot ","summary#Summary":"This post explains what prompts are in the context of GitHub Copilot Chat, how to create reusable prompts in the .github/prompts directory, and how to automatically include them in Copilot Chat using the github.copilot.chat.codeGeneration.instructions setting. An example and useful references are provided.\nFor more tips on writing effective prompts and using Copilot efficiently, see the Best Practices for Using GitHub Copilot section below.","understanding-prompts-in-github-copilot-chat#Understanding Prompts in GitHub Copilot Chat":"Prompts are instructions or context you provide to GitHub Copilot Chat to guide its code generation or responses. Well-crafted prompts help Copilot understand your intent, resulting in more accurate and relevant suggestions."},"title":"Prompts"},"/blog/runtimes/":{"data":{"":"In my current role as Head of Cloud Platform, I am leading the technical effort of migrating our entire on-premise real-estate to Azure. Part of this mission, is to upgrade the runtimes of our applications, regardless of their current placement; IIS Web apps, Windows Services and Docker Swarm containers. I say ‚Äúpart of this mission‚Äù as another aspect of this migration is to create a new foundation for our platform - AKS. I hope to cover more on this in later posts.\nWith being deliberately ambiguous on actual numbers here, we have a fair amount of workloads that are running on runtimes that have surpassed their end-of-life support status üò±. I am sure we‚Äôre not the only organization that finds itself in this predicament. One of the first things our CTO wanted when he came onboard was for us to sort out our runtimes.\nWhat this means is that we have .NET Framework and .NET Core workloads that need to be migrated to a runtime that is not out of support (now or anytime soon). At the same time, the target runtime has to have hit the mature (at least on it‚Äôs 1st minor release) status. .NET 6.0 is now GA but it‚Äôs paint is still a little wet. Understandably there is some concern, borderline trepidation towards targeting this new version. There are impactful benefits (improved performance, greater stability, improved GC, cost benefits resulting from being more perfomant, less vulnerabilities, improved security, etc‚Ä¶) and our current runtime target for our .NET Core workloads is 3.1. However, end-of-life support for this particular version is the end of this year - 03.Dec.2022. So, at some point, this upgrade needs to happen. With planning and prioritization, this can happen when appropriate.\nOne strategy of delivering confidence with a runtime is to update a few apps, consecutively; providing you‚Äôve availability to accommodate such an approach. Timeboxed: (1) A simple (CRUD-esque HTTP API that sits on top of a Db) workload then (2) one that involves an data-in-motion aspect (message queue, consumer paradigm, non-http api).\nAnother approach to reducing concern, mitigating risk, etc‚Ä¶ is to meet with the SME (Subject matter expert) - a Microsoft representative - and listen to what they have to say. I have orchestrated such a meeting - we are being helped/guided by Microsoft FastTrak. The advice received was to upgrade. We were also reminded of the migration paths to take when moving to a target version. Here are some demonstrative links that will help you through this process:\nMigration guidance found here ‚û° migrate from asp.net core 3.1 to 6.0.\nAn example of breaking changes between versions can be found here ‚û° breaking changes in .NET Core 3.1\nMost of our workloads are simple HTTP API CRUDs that sit aloft databases. In short, we‚Äôre not doing anything too complicated and so this makes most of our pain points the result of external NuGet package dependencies.\nIMO, it is important to establish, if at all possible, a feedback loop back to Microsoft. If you are fortunate to have an established relationship with Microsoft and have access to certain areas/teams - eg FastTrak, or even a Support plan, if/when issues do arise (eg increased exception frequency, uncharacteristic high latency, poor performance, frequent CG, memory leaks, socket exhaustion, etc‚Ä¶), you can feed these back and receive remediation advice until a fix (if not a result of poor implementation) becomes available. We had been plagued with issues (socket exhaustion, CG, memory leaks) with those workloads on ASP.NET Core 2.* but now see less issues with those ASP.NET Core workloads that are now on 3.1.\nTo complement the above I would also recommend agreeing on, and capturing, metrics that are demonstrative of your gains from the runtime upgrade. This is where metrics come into their own. Generally, metrics tend to have a greater retention period than logs, and therefore can paint an informed picture over a wider span of time than logging can. Generally speaking, lack of memory is going to be more catastrophic than thottling through lack of CPU. When your RSS or working set bytes are depleted, more often than not your application will terminate due to OOM (out of memory) exception - state and workflow blown away unless you‚Äôve employed patterns to mitigate against these edge/corner cases - most don‚Äôt. In particular, I am referring to container_memory_rss and container_memory_working_set_bytes due to working with kubernetes.\nThe above is not a definitive list of must do‚Äôs and in part, is overly simplistic. IMO, it is important to stay current with runtimes to benefit from the improvements they deliver on. For me though, their stability and the vulnerabilities they address is the most important. It is scary how many organizations run on runtimes that are out of support. Applications will still run but ask yourself this, how much is this truly costing your company, how much is it costing our environment and finally, how much could your company suffer because they are exposed to vulnerabilities?\nSounds overly dramatic doesn‚Äôt?! I stop there as I don‚Äôt wish to perpetuate the negativity of going down this particular hole. But before I do; more food for thought. I would like to throw into the mix application dependencies and in particular those package dependencies some of us rely on - NuGet \u0026 npm. Good DevOps practices mitigate again licensing or vulnerability scanning but IMO the same level of focus ought to be concentrated into this area as well. This was painfully brought to light recently regarding OSS and the log4j vulnerability - CVE-2021-44228 (common vulnerabilities and exposure). And finally, docker image scanning. There are plenty of solutions out there - Snyk for example - that can help with this. Good docker practices go a long way also toward protecting us against vulnerabilities that arise from images and not forgetting code scanning products generally that identify smelly code, bugs and other vulnerabilities. I have introduced solutions that deal with all of this."},"title":"Runtimes"},"/blog/shortcodes-img-with-caption/":{"data":{"":"I had an article I wanted to post but knew with the number of images I planned on including, it will look unsightly. The first idea I had to make it less so, was to indent the image to break up the page a little. The next idea I had was to include a caption under each image.\nKnowing that HTML has an element called figure, I decided to use this. I didn‚Äôt want anything complicated so I planned to keep the code to a minimum.\nWhen creating shortcodes, you add a file to the üìÅ layouts\\shortcodes\\ folder.\nThe name of this file is important. This name is what you used to invoke that shortcode. I decided to call my figure.\nI created this file and called it figure.html.\nI added my HTML. I‚Äôm using hugo expressions within the HTML to obtain the attribute values from the eventual declaration:\n\u003cfigure\u003e \u003cimg src=\"{{ .Get \"src\" }}\" alt=\"{{ .Get \"alt\" }}\"\u003e \u003cfigcaption\u003e{{ .Get \"caption\" }}\u003c/figcaption\u003e \u003c/figure\u003e To add a declaration to my markdown post, I would add something similar to this:\n{{\u003c figure src=\"../img/2024-02-25-10-54-40.png\" alt=\"\" caption=\".NET Aspire dashboard\" \u003e}} This particular example will produce this output:\n.NET Aspire dashboard You should see both an indentation and caption.\nThere you have it, a short and simple example of applying an indent and a caption to an image to break the page up.\nYou can see this shortcode being used here in this particular post ‚û°Ô∏è .NET Aspire and Redis.\nI‚Äôm not sure I‚Äôm happy with it‚Äôs name. On reflection, I will likely change this to caption."},"title":"Shortcode - An image with a caption"},"/blog/terraform-get-values/":{"data":{"":"","#":"With not wanting to have hard coded values pushed to a project‚Äôs code repository, and an antiquated way to derive Azure Service Principal credentials, I set about exploring ways on accomplishing this with this in mind using Terraform.\nAttempt 1 Here ins my first attempt, I load all permutations into map variables. I use an environment variable as an indexer to the appropriate map value:\nprovider \"azurerm\" { version = \"=2.17\" ... subscription_id = var.azure_subscription_id[var.environment] client_id = var.azure_client_id[var.environment] ... } variable \"azure_subscription_id\" { type = map default = { \"dev\" = \"********-****-****-****-************\" \"prod\"= \"********-****-****-****-************\" } } variable \"azure_client_id\" { type = map default = { \"dev\" = \"********-****-****-****-************\" \"prod\"= \"********-****-****-****-************\" } } ... Attempt 2 This second and more efficient approach, I used jsondecode function to load the entire credentials JSON to access the subscriptionId property:\nprovider \"azurerm\" { version = \"=2.17\" ... subscription_id = var.environment == \"dev\" ? jsondecode(var.azure_sp_dev).subscriptionId : jsondecode(var.azure_sp_prod).subscriptionId client_id = var.environment == \"dev\" ? jsondecode(var.azure_sp_dev).clientId : jsondecode(var.azure_sp_prod).clientId ... } variable \"azure_sp_dev\" { type = string default = \u003c\u003cEOT { \"clientId\": \"********-****-****-****-************\", \"clientSecret\": \"********-****-****-****-************\", \"subscriptionId\": \"********-****-****-****-************\", \"tenantId\": \"********-****-****-****-************\", \"activeDirectoryEndpointUrl\": \"https://login.microsoftonline.com\", \"resourceManagerEndpointUrl\": \"https://management.azure.com/\", \"activeDirectoryGraphResourceId\": \"https://graph.windows.net/\", \"sqlManagementEndpointUrl\": \"https://management.core.windows.net:8443/\", \"galleryEndpointUrl\": \"https://gallery.azure.com/\", \"managementEndpointUrl\": \"https://management.core.windows.net/\" } EOT } variable \"azure_sp_prod\" { type = string ... Obviously, none of the above deals with not pushing these credentials into the code repository.\nSo, in my opinion, there are 2 options available here. The first option is to use a GitHub Secret and to inject this secret into a script file. It could even be passed as a parameter to Terraform (e.g. terrafor apply -var credentials={...} ). Or, the second option is to obtain this key using the GitHub Azure/get-keyvault-secrets@v1.0 Action. This method will then allow you to obtain the Service Principal credentials from an Azure KeyVault. This latter approach means that we never need to expose these secrets outside of Azure, which we would have to do if we cut \u0026 paste them into a GitHub Secret."},"title":"Terraform Get Values"},"/blog/the-3rd-wave-of-cloud-computing/":{"data":{"how-easy-is-running-a-wasm-application-on-a-server#How easy is running a WASM application on a server?":"","in-summary#In Summary":"That‚Äôs quite an opening statement, isn‚Äôt it?\nAs a reminder, the 1st and 2nd waves were Virtual Machines and Containers, respectively.\nWarning, profound statement inbound‚Ä¶\nüåê The Docker cofounder, Solomon Hykes, said this in 2019\n‚ÄúIf WASM+WASI existed in 2008, we wouldn‚Äôt have needed to create Docker. That‚Äôs how important it is. WebAssembly on the server is the future of computing.‚Äù - Quote I think we need to let this sink in for a few moments. IMO, this is huge. Personally, since 2016, most of my DevOps [development \u0026 orchestration] work has involved containers. Container and FaaS is my preferred approach to development and orchestration. But now there‚Äôs a new way? Will I now have to throw out all this knowledge and return to ground zero and start over? In short, no. Anyways, back to this 3rd wave‚Ä¶\nWebAssembly (or WASM for short) is now considered to be the 3rd wave. I will cover what both WASM and WASI are. Before I do, I first want to put your mind at rest (and my own!). This post will not be leading to a conclusion were this technology will replace containers. Far from it. After all, did containers replace VMs? No, they didn‚Äôt. Neither will I be suggesting we have to migrate all our existing container-based solutions to this new shiny, WASM. Reassured? Good, let‚Äôs move on.\nWhat is WASM? So then, what is WASM? WebAssembly (or WASM) is a binary instruction format (Bytecode) that is designed to be a portable target for the compilation of high-level languages like JavaScript, C/C++, Rust, Python and .NET, enabling deployment on the web for either client or server applications. To run a WASM application you also require a WASM runtime.\nWASM isn‚Äôt a new technology either. It‚Äôs been around for several years and if you‚Äôve had any exposure to the .NET ecosystem then you‚Äôll most likely be aware of Blazor. However, all that Blazor is, is JavaScript running in a NodeJS console application with very little association to the wider WASM eco-system. For example, you could not take a compiled application module from Blazor and deploy them to Docker or Kubernetes. However, with .NET 8.0 there is the wasi-experimental workload that you can use in conjunction with the wasi-sdk and CLang. But as the name suggests, this is experimental and both specifications as well as the tooling are likely to change. I‚Äôm including this here to show that there is some WASM üíñ being given to .NET.\nWhat many of you (including myself up until a few months ago) may not be been aware of is that WASM can run outside of the browser (OOB) too; on a server. This is the very reason why I wanted to create this blog. This, and to make you aware of how profoundly this technology will impact how we will develop, and platform, our future applications.\nWASM by design is sandboxed. This is intentional. So, what does this mean? It means it can‚Äôt directly access resources on the host machine. It can only perform actions that it‚Äôs allowed to do within that sandbox. This is rigorously enforced. It can do one of two things. Let‚Äôs call these capabilities instead of ‚Äúthings‚Äù, after all, we‚Äôre all grown-ups? It‚Äôs first capability is it can only perform operations on its own memory. This by design is purposely constraining so for it to be of any real use it has to offer up a method of allowing I/O with its host. And this is its second capability so making it possible to interact with its host. This is made possible using Import \u0026 export functions. Export functions allow the host to call into a WASM application. Conversely, Import functions allows the WASM application to call into the Host. This second capability is where a new technology comes into play - WASI. Which leads us nicely into what else is needed‚Ä¶\nWhat else is needed? Now that we have touched on the second capability or Import \u0026 Export functions, we may now want to think about where these might sit as well as why do we need these.\nFor example, you may want to access the host‚Äôs filesystem (or what the host would like you to think is its filesystem) or to read the system clock. Infact there are a range of resources your WASM application may want to access from its host. This access will need to be performed irrespective of what O/S you‚Äôre on, or what its instruction set is. This range of access is known as the API. To help in the development of this API and to ensure there‚Äôs an associated standard, the Bytecode Alliance was formed and became responsible for driving this forward. The Bytecode Alliance consists of an alliance of many organisations, as seen here:\nList of all the companies that form the Bytecode Alliance\nWhat is being developed is a specification that is a standardized system interface for WASM that enables it to interact with the underlying operating system in a secure and platform independent manner.\nThe WASI logo\nThis is known as WASI (Web Assembly Standard Interface). WASI provides a single standard way to call the low-level functions that are present on any platform, making it easier to write software once and run it anywhere.\n‚ö†Ô∏è WASI specifically relates to running WASM on a server and has nothing to do with running WASM in the browser.\nThere‚Äôs an important point to make known and this is, just because the spec says you can access the host‚Äôs filesystem, by default, you can‚Äôt. The host must first allow you to access it‚Äôs filesystem. This ensures that the sandbox remains rigorously enforced.\nSo, we have the application that has been compiled into WASM bytecode, and we need to use the WASI API to be able to access the host‚Äôs resources. But we also need something to actually run the WASM application on the host. This is known as the WASM runtime and there are several runtimes available. The runtimes available from the Bytecode Alliance are wasmtime and WAMR. wasmtime is a high-performance runtime for the server. WAMR targets more resource constrained scenarios such as IoT. The runtime that I have been using recently is wasmtime. This, however, is abstracted away from me through the toolchain. I have been using the spin toolchain from Fermyon.\nWhat are the benefits?So far, I‚Äôve described what WASM is, what it needs to offer up any real value as well as how it gets executed on the host. What I am yet to touch on is why anybody should invest their time in this technology. Let‚Äôs address this now by highlighting the main benefits of WASM.\nWASM‚Äôs benefits aren‚Äôt too dissimilar to those of containerization. Here‚Äôs the list:\nIt has near native performance without sacrificing safety. It supports multiple programming languages including emerging languages like Rust. If a WASM application crashes, it will not bring down anything else running on that same machine. It has a smaller attack surface and a simpler software supply chain than Linux containers. There‚Äôs no possibility of malicious data leaks or the exfiltration of data. It is ideal for serverless use-cases and those event-based scenario; once run, then it can scale to zero. It is more portable than containers as the same WASM Bytecode works the same across every machine so therefore it is architectural independent. It‚Äôs very light weight, more so than docker. An application can be as small as a few KBs. In comparison, each of your docker processes are effectively running it‚Äôs own O/S. An example size can be seen here: Image as shown in DockerHub, confirming OS \u0026 Architecture as WASI \u0026 WASM\nThere is other functionality that is planned. For example, in the next 12 months we will likely see greater emphasis made around the component model. The component model is a way to define and compose WASM modules so that they can interact with each other, and the host environment. This will enable WASM modules to be reusable, portable, secure, and efficient across different platforms and fulfil a richer set of use cases.\nWhy is this interesting?For me, somebody who has a genuine passion for Sustainability, and in particular, Sustainability Transformation, it‚Äôs speaks to me at a GSF principle level - Energy Efficiency in particular. Meaning that we must aim to use as little as energy as possible, essentially getting the most out of each unit of energy. I feel that this 3rd wave is a perfect fit for this principle. This is providing that the bin packing/right-sizing capability of the hosts is maximized, otherwise it‚Äôs just another application rattlingly around in the server that by default will always use a set amount of energy and emit a set amount of carbon.\nAs an FYI, I gave a talk on Sustainability Transformation during our Fujitsu PROSummit as well as at the National DevOps Conference in October in a Breakout session with a colleague. All talks I give will have an aspect of sustainability associated with them.\nWhat also reignited my interest in WASM was when I researched WASM with my fellow Professions colleagues. This was at the request of our CDO and to produce an executive summary a few months back. Since, I‚Äôve continued my research in this area, even writing some C code which I‚Äôve not done since 1995! This code can also be found in another public GH repo of mine.\nHow easy is running a WASM application on a server?I‚Äôve experience of using a few toolchains in this area. The toolchain I‚Äôm using presently comes from Fermyon and this is called spin. I‚Äôve a GitHub code repository here https://github.com/garrardkitchen/docker-wasm-spin that, if you‚Äôre interested, will get you up and running in a few minutes. If you know me or have seen any of my talks on the Professions platform, you‚Äôll know I always accompany a talk with a GitHub code repository. Fermyon also have a cloud offering that you can deploy directly to. You can also deploy your WASM application to your own Kubernetes cluster. My code repository covers running a spin WASM application on Docker for Desktop and it is a TypeScript HTTP Trigger taken directly from a spin template. The instructions to build, run and deploy are in the README\nHere‚Äôs a picture of my running container on Docker for Desktop (see the WASM tag):\nShowing a WASM container running on Docker for Desktop\nAn engineer coding a serverless function in WASM\nMy sample code is ultra basic and all it does is run an HTTP listener. You could add a KV store or Db connection to extend this example to make it more realistic.\nOne fun activity you could participate in is to review some of the Azure Functions or AWS Lambdas or GCP Cloud Functions you have recently written and assess which of these could have been done in WASM?\nYou could even go one step farther and rewrite one of these in WASM then share the link in the Comment section below. I would genuinely love to see this!\nThe OSS communityI did hit a problem when I first dipped my toe into spin, that the good people from the Fermyon community in Discord helped me with. I do make a point of reaching out to the community of any open-source project I am looking into, and I use this as a barometer as to the quality and longevity of their product. I have to say, they didn‚Äôt disappoint. Several members helped me, including their CEO!\nIn SummaryIn summary, WASM on the server is important for developing modern, secure, and efficient applications that can run closer to the data and the users. It is especially useful for microservices, edge computing and cloud-native computing.\nI do hope you have found this article interesting and seen how uncomplicated it is to deploy having cloned my GitHub code repository. I also hope this has piqued your interest to the point that you‚Äôre already started exploring this fascinating ecosystem. It‚Äôs an area of cloud computing that I‚Äôm excited about and looking forward to it being mainstream. It is clear to see that WASM/WASI will be a cloud disruptor, and that a delivery cycle involving this technology can be ridiculously short, not to mention energy efficient.","the-oss-community#The OSS community":"","what-are-the-benefits#What are the benefits?":"","what-else-is-needed#What else is needed?":"","what-is-wasm#What is WASM?":"","why-is-this-interesting#Why is this interesting?":""},"title":"The 3rd Wave of Cloud Computing"},"/blog/the-grass-is-rarely-greener/":{"data":{"considerations-when-moving-to-another-cloud-provider#Considerations when moving to another Cloud Provider":"I recently had a conversation that stirred up some surpressed memories. The conversation was related to moving to a different cloud vendor. That journey didn‚Äôt work out so well for me and the company I was working for at the time. Sadly, that company stopped trading and I don‚Äôt think it‚Äôs that much of a leap to link that journey to the demise of that company. Hopefully, now the title of this post is starting to make more sense?\nSeveral years back, the company that I was working for at the time shifted to AWS, away from Azure. It was heavily suggested - I was present during that conversation - that it would be in our interest if we ‚Äújumped ship‚Äù to their platform. ‚ÄúTheir‚Äù being ‚ÄúAWS‚Äù. Long story short, sadly it was not. In fact, it turned out to be a huge waste of time. IMO, they should never have suggested this. There were other suggestions made that they really should not have made; sadly we followed these too. In retrospect, it was totally unjustifiable not to mention unfair for them to suggest any of this for a company of that size [small], especially knowing how long all of this would take to do. I even gained 2 AWS certifications to help best advise the company on how to implement/integrate with the AWS Platform and backing services.\nIf you are thinking, ‚ÄúYou are grown-ups with years of experience under your belt yeah, so you didn‚Äôt have to go with what they said did you?‚Äù. Indeed, but it was AWS and these people we were talking to were big hitters and as such, the ‚Äúcarrot‚Äù was big. Who wouldn‚Äôt right?!\nI will return to this in a later post but for now I will call out, IMO, several important and possibly overlooked considerations when contemplating a move to a different Cloud Provider.\nConsiderations when moving to another Cloud ProviderHere are a few nuggets that will hopefully resonate:\n1 - I don‚Äôt know of any business that will allow their ‚Äútechnical people‚Äù to have their way and purely focus on tech debt or similar, without developing features (or improvements) over a prolonged amount of time. If part funded with VC money, I‚Äôm sure they won‚Äôt be too happy either - further delay on their return, and for what? With all the best intentions in the world, migrations are time-consuming (not to mention a scheduling nightmare), and this time is exponential when dealing with new or unfamiliar tech. Tech will also present problems, regardless of where it is.\n2 - Cloud vendors offer essentially the same backing services - orchestration, serverless compute, VM compute, db, caching, temporal decoupling thru queues, and the list goes on‚Ä¶ . They may make it easier or more attractive for particular language techs and ecosystems, but essentially, it‚Äôs the same offerings plus patterns (EIP but in the Cloud) but elsewhere. Not to mention they all have their own version of the 5 pillars of the ‚ÄúWell-Architecture‚Äù framework.\n3 - As an extension of (2) above, there‚Äôs training to consider. Also, new problems to solve, and migration paths to figure out. Not to mention RBAC and ownership. And don‚Äôt forget DevOps, possibly made slightly easier if you‚Äôve opted to a non-native IaC approach like TF (Terraform). And don‚Äôt forget automation, runbooks, alerting, 3 pillars of observation (incl. distributed tracing). And don‚Äôt forget‚Ä¶ .\n4 - If there‚Äôs something that your current cloud vendor doesn‚Äôt offer and the need is absolutely justifiable, you can also look for a SaaS alternative. When you boil it down, all you are really interested in are specific use-cases/features, integration options (SDKs) and it being a ‚Äúmanaged service‚Äù as to avoid requiring additional Engineers to manage it and perform associated activities to ensure it‚Äôs HA, patched and secure. Even if it‚Äôs a service only offered by a different cloud vendor, there are light-touch integration options - for example, AWS Lambda and AWS Kinesis Data Streaming. This is an approach we‚Äôve had to take.\n5 - It‚Äôs always going to take longer than you think. Fact. We tend to think ‚Äúhappy path‚Äù.\n6 - Now, this next one is a biggy ‚Ä¶ if you are moving to new tech, whether it be a new cloud vendor but more appropriately shifting to a new server-side language (eg from c# to go), you are going to lose between 20-40% of those Engineers who‚Äôs core language is c#. I‚Äôve not observed the same levels of attrition on a shift to a different client-side tech. More often, you‚Äôll have several client-side techs in play. Maybe some core concepts of newly opted JS framework have changed, but essentially the language tech hasn‚Äôt (JS/TS). Well, that‚Äôs if you‚Äôre not shifting to Blazor (WebAssembly). One major deficit of when your Engineers leave is that so does their domain knowledge üò±. Again, speaking from a position of experience, it‚Äôs sad to see people go. Morale takes a hit. And when Engineers leave, their roles need replacing too. Recruitment ensues, which requires planning plus patience, often the cause of disruptions and then it takes other Engineers‚Äô time for onboarding and orientation.\n7 - If cost is a consideration or motivation for the move then what are you doing re: the Cost Optimisation tenet of your provider‚Äôs flavour of a ‚ÄúWell-Architected‚Äù Framework? More important than this but less obvious, what is the Cloud Service Provider (CSP) or organisation that is the conduit between you and the Cloud Vendor doing to help? In my experience, they only get involved if you need help with an issue, or if there‚Äôs a zero-day exploit you need to be made away of or other type of issue involving one of your backing services. Anything difficult, then they tend to hand off to the Cloud Vendor themselves, but hey, at least they‚Äôre on the same support call with you! üëÄ. They generally don‚Äôt get in your face either if you‚Äôre spending a futune! You may get the occasional email from them letting you know that you are very important to them. IMO, they need to be continually proving their worth to you [your organisation] and if they‚Äôre not, engage a different CSP or employ certified professional(s) who can evaluate your architecture, and your DevOps processes as well as costs. Essentially, you need certified/experienced Engineers to help to make real tangible contributions and improvements.\n8 - If technical issues are plaguing you, then likely they‚Äôll manifest themselves also on a different cloud provider. So, if you‚Äôve invested heavily with a particular tech, or approach, and this is too costly, too risky to the business due to instability, first understand why first before jumping ship. You never know, you might be using it inefficiently or incorrectly. Take AKS for instance. It‚Äôs hugely involved. If you get any part of that wrong, it‚Äôs a risk to your business.\nThere will be more I‚Äôm sure, but for the sake of getting closure on this topic and to put these emotions back into that box, I will leave it there.","recommendations#Recommendations":"In summary, my advice is this; evaluate your current platform to identify (in no particular order):\nCost savings (eg there could be better bin packing or scaling options or up to 70% reduction on ‚Äúcompute‚Äù by using Reserved Instances) Configuration changes to improve stability, minimize failure on upgrading your dependencies, and self-healing Improvements on patterns and practices used by your Engineers (incl. low code options, delegate failure edge/corner cases to your architecture, consistency of solution - don‚Äôt solve the same problem multiple ways!, only measure what you can action) Automation (eg runbook when metric exceeded, cron jobs) What is your prominent tech? For example, if your ecosystem is predominately .NET then Azure is a justifiable Cloud Platform. You simply won‚Äôt get the same depth/scope of features/integrations if you go with a different Cloud Provider. Think about it‚Ä¶would you offer up all your crown jewels to a competing provider? I rest my case! I do hope this post has been informative. If I help one person avoid this more often than not wasted effort, then this post will have been worth it. Changes of this magnitude are laden with risk and one thing companies are adverse to is risk."},"title":"The Grass Is Rarely Greener"},"/docs/":{"data":{"":"üëã Hello! Welcome to the Hextra documentation!","features#Features":" Beautiful Design - Inspired by Nextra, Hextra utilizes Tailwind CSS to offer a modern design that makes your site look outstanding. Responsive Layout and Dark Mode - It looks great on all devices, from mobile, tablet to desktop. Dark mode is also supported to accommodate various lighting conditions. Fast and Lightweight - Powered by Hugo, a lightning-fast static-site generator housed in a single binary file, Hextra keeps its footprint minimal. No JavaScript or Node.js are needed to use it. Full-text Search - Built-in offline full-text search powered by FlexSearch, no additional configuration required. Battery-included - Markdown, syntax highlighting, LaTeX math formulae, diagrams and Shortcodes elements to enhance your content. Table of contents, breadcrumbs, pagination, sidebar navigation and more are all automatically generated. Multi-language and SEO Ready - Multi-language sites made easy with Hugo‚Äôs multilingual mode. Out-of-the-box support is included for SEO tags, Open Graph, and Twitter Cards. ","next#Next":"Dive right into the following section to get started:\nGetting StartedLearn how to create website using Hextra ","questions-or-feedback#Questions or Feedback?":" ‚ùì Hextra is still in active development. Have a question or feedback? Feel free to open an issue! ","what-is-hextra#What is Hextra?":"Hextra is a modern, fast and batteries-included [Hugo][hugo] theme built with [Tailwind CSS][tailwind-css].\nDesigned for building beautiful websites for documentation, blogs, and websites, it provides out-of-the-box features and flexibility to meet various requirements."},"title":"How To"},"/docs/advanced/":{"data":{"":"This section covers some advanced topics of the theme.\nMulti-language Customization Comments System "},"title":"Advanced"},"/docs/advanced/comments/":{"data":{"":"Hextra supports adding comments system to your site. Currently giscus is supported.","giscus#giscus":"giscus is a comments system powered by GitHub Discussions. It is free and open source.\nTo enable giscus, you need to add the following to the site configuration file:\nhugo.yamlparams: comments: enable: false type: giscus giscus: repo: \u003crepository\u003e repoId: \u003crepository ID\u003e category: \u003ccategory\u003e categoryId: \u003ccategory ID\u003e The giscus configurations can be constructed from the giscus.app website. More details can also be found there.\nComments can be enabled or disabled for a specific page in the page front matter:\ncontent/docs/about.md--- title: About comments: true --- "},"title":"Comments"},"/docs/advanced/customization/":{"data":{"":"Hextra offers some default customization options in the hugo.yaml config file to configure the theme. This page describes the available options and how to customize the theme further.","custom-css#Custom CSS":"To add custom CSS, we need to create a file assets/css/custom.css in our site. Hextra will automatically load this file.\nFont Family The font family of the content can be customized using:\nassets/css/custom.css.content { font-family: \"Times New Roman\", Times, serif; } Inline Code Element The color of text mixed with other text can customized with:\nassets/css/custom.css.content code:not(.code-block code) { color: #c97c2e; } Primary Color The primary color of the theme can be customized by setting the --primary-hue, --primary-saturation and --primary-lightness variables:\nassets/css/custom.css:root { --primary-hue: 100deg; --primary-saturation: 90%; --primary-lightness: 50%; } Further Theme Customization The theme can be further customized by overriding the default styles via the exposed css classes. An example for customizing the footer element:\nassets/css/custom.css.hextra-footer { /* Styles will be applied to the footer element */ } .hextra-footer:is(html[class~=\"dark\"] *) { /* Styles will be applied to the footer element in dark mode */ } The following classes can be used to customize various parts of the theme.\nGeneral hextra-scrollbar - The scrollbar element content - Page content container Shortcodes Badge hextra-badge - The badge element Card hextra-card - The card element hextra-card-image - The card image element hextra-card-icon - The card icon element hextra-card-subtitle - The card subtitle element Cards hextra-cards - The cards grid container Jupyter Notebook hextra-jupyter-code-cell - The Jupyter code cell container hextra-jupyter-code-cell-outputs-container - The Jupyter code cell outputs container hextra-jupyter-code-cell-outputs - The Jupyter code cell output div element PDF hextra-pdf - The PDF container element Steps steps - The steps container Tabs hextra-tabs-panel - The tabs panel container hextra-tabs-toggle - The tabs toggle button Filetree hextra-filetree - The filetree container Folder hextra-filetree-folder - The filetree folder container Navbar nav-container - The navbar container nav-container-blur - The navbar container in blur element hamburger-menu - The hamburger menu button Footer hextra-footer - The footer element hextra-custom-footer - The custom footer section container Search search-wrapper - The search wrapper container search-input - The search input element search-results - The search results list container Table of Contents hextra-toc - The table of contents container Sidebar mobile-menu-overlay - The overlay element for the mobile menu sidebar-container - The sidebar container sidebar-active-item - The active item in the sidebar Language Switcher language-switcher - The language switcher button language-options - The language options container Theme Toggle theme-toggle - The theme toggle button Code Copy Button hextra-code-copy-btn-container - The code copy button container hextra-code-copy-btn - The code copy button Code Block hextra-code-block - The code block container Feature Card hextra-feature-card - The feature card link element Feature Grid hextra-feature-grid - The feature grid container Breadcrumbs No specific class is available for breadcrumbs.\nSyntax Highlighting List of available syntax highlighting themes are available at Chroma Styles Gallery. The stylesheet can be generated using the command:\nhugo gen chromastyles --style=github To override the default syntax highlighting theme, we can add the generated styles to the custom CSS file.","custom-extra-section-in-footer#Custom Extra Section in Footer":"You can add extra section in the footer by creating a file layouts/partials/custom/footer.html in your site.\nlayouts/partials/custom/footer.html\u003c!-- Your footer element here --\u003e The added section will be added before the copyright section in the footer. You can use HTML and Hugo template syntax to add your own content.\nHugo variables available in the footer section are: .switchesVisible and .displayCopyright.","custom-layouts#Custom Layouts":"The layouts of the theme can be overridden by creating a file with the same name in the layouts directory of your site. For example, to override the single.html layout for docs, create a file layouts/docs/single.html in your site.\nFor further information, refer to the [Hugo Templates][hugo-template-docs].","custom-scripts#Custom Scripts":"You may add custom scripts to the end of the head for every page by adding the following file:\nlayouts/partials/custom/head-end.html ","further-customization#Further Customization":"Didn‚Äôt find what you were looking for? Feel free to open a discussion or make a contribution to the theme!"},"title":"Customization"},"/docs/advanced/multi-language/":{"data":{"":"Hextra supports creating site with multiple languages using Hugo‚Äôs multilingual mode.","enable-multi-language#Enable Multi-language":"To make our site multi-language, we need to tell Hugo the supported languages. We need to add to the site configuration file:\nhugo.yamldefaultContentLanguage: en languages: en: languageName: English weight: 1 fr: languageName: Fran√ßais weight: 2 ja: languageName: Êó•Êú¨Ë™û weight: 3 ","manage-translations-by-filename#Manage Translations by Filename":"Hugo supports managing translations by filename. For example, if we have a file content/docs/_index.md in English, we can create a file content/docs/_index.fr.md for French translation.\ncontent docs _index.md _index.fr.md _index.ja.md Note: Hugo also supports Translation by content directory.","read-more#Read More":" Hugo Multilingual Mode Hugo Multilingual Part 1: Content translation Hugo Multilingual Part 2: Strings localization ","translate-menu-items#Translate Menu Items":"To translate menu items in the navigation bar, we need to set the identifier field:\nhugo.yamlmenu: main: - identifier: documentation name: Documentation pageRef: /docs weight: 1 - identifier: blog name: Blog pageRef: /blog weight: 2 and translate them in the corresponding i18n file:\ni18n/fr.yamldocumentation: Documentation blog: Blog ","translate-strings#Translate Strings":"To translate strings on the other places, we need to add the translation to the corresponding i18n file:\ni18n/fr.yamlreadMore: Lire la suite A list of strings used in the theme can be found in the i18n/en.yaml file."},"title":"Multi-language"},"/docs/getting-started/":{"data":{"":"","next#Next":"Explore the following sections to start adding more contents:\nOrganize Files Configuration Markdown ","quick-start-from-template#Quick Start from Template":" imfing/hextra-starter-template\nYou could quickly get started by using the above template repository.\nWe have provided a GitHub Actions workflow which can help automatically build and deploy your site to GitHub Pages, and host it for free. For more options, check out Deploy Site.\nüåê Demo ‚Üó","start-as-new-project#Start as New Project":"There are two main ways to add the Hextra theme to your Hugo project:\nHugo Modules (Recommended): The simplest and recommended method. Hugo modules let you pull in the theme directly from its online source. Theme is downloaded automatically and managed by Hugo.\nGit Submodule: Alternatively, add Hextra as a Git Submodule. The theme is downloaded by Git and stored in your project‚Äôs themes folder.\nSetup Hextra as Hugo module Prerequisites Before starting, you need to have the following software installed:\nHugo (extended version) Git Go Steps Initialize a new Hugo site hugo new site my-site --format=yaml Configure Hextra theme via module # initialize hugo module cd my-site hugo mod init github.com/username/my-site # add Hextra theme hugo mod get github.com/imfing/hextra Configure hugo.yaml to use Hextra theme by adding the following:\nmodule: imports: - path: github.com/imfing/hextra Create your first content pages Create new content page for the home page and the documentation page:\nhugo new content/_index.md hugo new content/docs/_index.md Preview the site locally hugo server --buildDrafts --disableFastRender Voila, your new site preview is available at http://localhost:1313/.\nHow to update theme? To update all Hugo modules in your project to their latest versions, run the following command:\nhugo mod get -u To update Hextra to the latest released version, run the following command:\nhugo mod get -u github.com/imfing/hextra See Hugo Modules for more details.\nSetup Hextra as Git submodule Prerequisites Before starting, you need to have the following software installed:\nHugo (extended version) Git Steps Initialize a new Hugo site hugo new site my-site --format=yaml Add Hextra theme as a Git submodule git submodule add https://github.com/imfing/hextra.git themes/hextra Configure hugo.yaml to use Hextra theme by adding the following:\ntheme: hextra Create your first content pages Create new content page for the home page and the documentation page:\nhugo new content/_index.md hugo new content/docs/_index.md Preview the site locally hugo server --buildDrafts --disableFastRender Your new site preview is available at http://localhost:1313/.\nWhen using CI/CD for Hugo website deployment, it‚Äôs essential to ensure that the following command is executed before running the hugo command.\ngit submodule update --init Failure to run this command results in the theme folder not being populated with Hextra theme files, leading to a build failure.\nHow to update theme? To update all submodules in your repository to their latest commits, run the following command:\ngit submodule update --remote To update Hextra to the latest commit, run the following command:\ngit submodule update --remote themes/hextra See Git submodules for more details."},"title":"Getting Started"},"/gaming/":{"data":{"":"It‚Äôs likely that a majority of the guidance found behind this link is the consequence of me helping my 2 boys deal with various gaming related issues."},"title":"Gaming"},"/gaming/minecraft/java-edition/":{"data":{"":"TL;DR: ¬£70 lighter, shared family learning and happiness (well, kinda)\nTBC‚Ä¶"},"title":"I am not Steve!"},"/nuggets/":{"data":{"":"Behind this link there re links and snippets that don‚Äôt naturally fit into a Blog or a How To and generally feel more of a link to some other reason or a snippet of info that helps or shows you an individual feature."},"title":"Nuggets"},"/nuggets/mcp/mcp-local/":{"data":{"":"","links#Links":""},"title":"Local"},"/nuggets/mcp/mcp-remote/":{"data":{"":"","links#Links":"Links "},"title":"Remote"},"/nuggets/mcp/mcp-vscode/":{"data":{"":"","links#Links":"Links "},"title":"VSCode"},"/nuggets/prompts/":{"data":{"":"This post explains what prompts are in the context of GitHub Copilot Chat"},"title":"Prompts"}}